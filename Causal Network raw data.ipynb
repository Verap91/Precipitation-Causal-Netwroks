{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ef92d8",
   "metadata": {},
   "source": [
    "Seasonal Causal network for raw data\n",
    "\n",
    "- Jarque-Bera residual test\n",
    "- building networks\n",
    "- visualizing network nodes (ingrid, outgrid)\n",
    "- analysing edges attributes ( number of connections, strenght, direction, spatial lenght)\n",
    "- network analysis: Clustering Coefficient and Average Shortest Path Length Across Year\n",
    "- PDF and Decumulative distribution of GC Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals and networks\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from scipy.stats import jarque_bera, norm\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data_filtered = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data_filtered['DATETIME'] = pd.to_datetime(merged_data_filtered['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 683, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data = merged_data_filtered[merged_data_filtered['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "\n",
    "# Function to perform Granger causality test and manual residual computation\n",
    "def granger_test_manual(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        # Create lagged version of the independent variable (column1)\n",
    "        X = dataframe[[column1]].shift(max_lag).dropna()\n",
    "        y = dataframe[column2].iloc[max_lag:]  # Dependent variable adjusted for shift\n",
    "        X = X[:len(y)]  # Align lengths of X and y\n",
    "\n",
    "        # Add constant to the model\n",
    "        X = add_constant(X)\n",
    "\n",
    "        # Fit OLS regression model\n",
    "        model = OLS(y, X).fit()\n",
    "\n",
    "        # Calculate residuals\n",
    "        residuals = model.resid\n",
    "\n",
    "        # Perform Jarque-Bera test for normality on residuals\n",
    "        jb_stat, jb_p_value = jarque_bera(residuals)\n",
    "        normality = \"Normal\" if jb_p_value > 0.05 else \"Not normal\"\n",
    "\n",
    "        # Plot residuals for normal and non-normal cases\n",
    "        plot_residuals(residuals, normality, column1, column2)\n",
    "\n",
    "        # Return F-statistic\n",
    "        f_statistic = model.fvalue\n",
    "        return f_statistic if model.f_pvalue < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test_manual: {e}\")\n",
    "        return 0  # Return 0 in case of an error\n",
    "\n",
    "# Function to plot residuals\n",
    "def plot_residuals(residuals, normality, column1, column2):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot histogram of residuals\n",
    "    plt.hist(residuals, bins=20, density=True, alpha=0.6, color='g', label=\"Residuals\")\n",
    "\n",
    "    # Fit a normal distribution to the residuals\n",
    "    mu, std = norm.fit(residuals)\n",
    "\n",
    "    # Plot the normal distribution\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    plt.plot(x, p, 'k', linewidth=2, label=\"Normal fit\")\n",
    "\n",
    "    # Title and labels\n",
    "    plt.title(f'Residuals Distribution: {column1} -> {column2} ({normality})')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "# Find global maxima for in-links and F-statistics across all years\n",
    "global_max_links = 0\n",
    "global_max_f_statistic = 0\n",
    "\n",
    "for year in range(merged_data['DATETIME'].dt.year.min(), merged_data['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data[(merged_data['DATETIME'].dt.year == year) & (merged_data['DATETIME'].dt.month.isin([9, 10, 11]))]  # Filter for March, April, May\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "\n",
    "    # Initialize temporary dictionaries for this year\n",
    "    temp_in_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum = {station: 0 for station in pivot_data.columns}\n",
    "\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test_manual(pivot_data, station1, station2)\n",
    "                if f_statistic > 0:\n",
    "                    temp_in_links_count[station2] += 1\n",
    "                    temp_f_statistics_sum[station2] += f_statistic\n",
    "\n",
    "    # Update global maxima if necessary\n",
    "    year_max_links = max(temp_in_links_count.values())\n",
    "    year_max_f_statistic = max(temp_f_statistics_sum.values())\n",
    "    global_max_links = max(global_max_links, year_max_links)\n",
    "    global_max_f_statistic = max(global_max_f_statistic, year_max_f_statistic)\n",
    "\n",
    "# Loop over each year for network creation and visualization\n",
    "for year in range(merged_data['DATETIME'].dt.year.min(), merged_data['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data[(merged_data['DATETIME'].dt.year == year) & (merged_data['DATETIME'].dt.month.isin([9, 10, 11]))]  # Filter for March, April, May\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "\n",
    "    # Initialize dictionaries for in-links and sum of F-statistics for incoming links\n",
    "    in_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    incoming_f_statistics_sum = {station: 0 for station in pivot_data.columns}\n",
    "\n",
    "    # Create network\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(pivot_data.columns)\n",
    "\n",
    "    # Add edges and count in-links and F-statistics for incoming links\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test_manual(pivot_data, station1, station2)\n",
    "                if f_statistic > 0:\n",
    "                    G.add_edge(station1, station2)\n",
    "                    in_links_count[station2] += 1\n",
    "                    incoming_f_statistics_sum[station2] += f_statistic\n",
    "\n",
    "    # Normalize node sizes based on the sum of incoming F-statistics\n",
    "    scaling_factor = 150\n",
    "    node_sizes = [incoming_f_statistics_sum[station] * scaling_factor / global_max_f_statistic for station in G.nodes()]\n",
    "\n",
    "    # Normalize and set node colors based on in-links count\n",
    "    norm = Normalize(vmin=0, vmax=global_max_links)\n",
    "    cmap = plt.cm.coolwarm\n",
    "    color_map = ScalarMappable(norm=norm, cmap=cmap)\n",
    "    node_colors = [color_map.to_rgba(in_links_count[station]) for station in G.nodes()]  # Visualization with scaled node sizes and color gradient\n",
    "    pos = {station: (coordinates_data[coordinates_data['STATION'] == station]['EST'].iloc[0],\n",
    "                     coordinates_data[coordinates_data['STATION'] == station]['NORD'].iloc[0])\n",
    "           for station in G.nodes()}\n",
    "\n",
    "    nx.draw(G, pos, with_labels=True, node_size=node_sizes, node_color=node_colors, node_shape=\"o\", edge_color='none', font_size=7)\n",
    "    plt.title(f\"Network of Gauges ({year}) with Node Sizes and Colors Based on Incoming Links (March-May)\")\n",
    "    #plt.savefig(f\"4.9gauges_innetwork_{year}.jpg\", dpi=300)  # Save as JPG\n",
    "    plt.close()  # Close the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ingrid\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.patches import Circle\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data_filtered['DATETIME'] = pd.to_datetime(merged_data_filtered['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 683, 695, 718,756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data = merged_data_filtered[merged_data_filtered['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "\n",
    "station_year_data = {}\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=lag)\n",
    "        f_statistic = gc_test[lag][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[lag][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.0001 else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data['DATETIME'].dt.year.min(), merged_data['DATETIME'].dt.year.max() + 1):\n",
    "    # Filter data for months 3, 4, and 5\n",
    "    #data_year = merged_data[(merged_data['DATETIME'].dt.year == year) & \n",
    "                            #(merged_data['DATETIME'].dt.month.isin([6, 7, 8]))]\n",
    "    \n",
    "    data_year = merged_data[(merged_data['DATETIME'].dt.year == year)]\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "\n",
    "    temp_in_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum = {station: 0 for station in pivot_data.columns}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test(pivot_data, station1, station2, lag=1)  # Change lag value here\n",
    "                if f_statistic > 0:\n",
    "                    temp_in_links_count[station2] += 1\n",
    "                    temp_f_statistics_sum[station2] += f_statistic\n",
    "                    station_year_data[(station2, year)] = (temp_f_statistics_sum[station2], temp_in_links_count[station2])\n",
    "\n",
    "    # Update global maxima\n",
    "    year_max_links = max(temp_in_links_count.values())\n",
    "    year_max_f_statistic = max(temp_f_statistics_sum.values())\n",
    "    global_max_links = max(global_max_links, year_max_links)\n",
    "    global_max_f_statistic = max(global_max_f_statistic, year_max_f_statistic)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {\n",
    "    779: \"TP\",\n",
    "    750: \"PA\",\n",
    "    683: \"AG\",\n",
    "    695: \"CL\",\n",
    "    718: \"EN\",\n",
    "    756: \"RG\",\n",
    "    706: \"CT\",\n",
    "    764: \"SR\",\n",
    "    729: \"ME\"\n",
    "}\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "import numpy as np\n",
    "\n",
    "# Supponiamo che station_year_data, station_order, global_max_f_statistic, e global_max_links siano già definiti\n",
    "\n",
    "# Crea la visualizzazione della griglia 2D\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.set_facecolor('white')  # Imposta lo sfondo bianco\n",
    "\n",
    "# Definisce le scale per la dimensione e il colore\n",
    "max_node_size = .2  # Massima dimensione del nodo\n",
    "color_norm = plt.Normalize(0, global_max_links)  # Normalizza il conteggio dei collegamenti\n",
    "color_map = plt.cm.coolwarm  # Mappa dei colori\n",
    "\n",
    "# Disegna ogni stazione-anno come un cerchio sulla griglia\n",
    "for (station, year), (f_stat, links) in station_year_data.items():\n",
    "    x = year\n",
    "    y = list(station_order.keys()).index(station)  # Ottiene la posizione della stazione in base all'ordine\n",
    "    size = (f_stat / global_max_f_statistic) * max_node_size  # Scala la dimensione in base a f_stat\n",
    "    color = color_map(color_norm(links))  # Ottiene il colore in base al numero di collegamenti\n",
    "\n",
    "    # Crea un cerchio e aggiungilo al grafico\n",
    "    circle = Circle((x, y), np.sqrt(size), color=color, alpha=0.6)  # Usa la radice quadrata della dimensione per il raggio\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "# Imposta etichette degli assi, tacche e limiti\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Station ID')\n",
    "ax.set_xticks(np.arange(merged_data['DATETIME'].dt.year.min(), merged_data['DATETIME'].dt.year.max() + 1))\n",
    "ax.set_yticks(np.arange(len(station_order)))\n",
    "ax.set_yticklabels(station_order.values())\n",
    "ax.set_xlim(merged_data['DATETIME'].dt.year.min() - 1, merged_data['DATETIME'].dt.year.max() + 1)\n",
    "ax.set_ylim(-1, len(station_order))\n",
    "\n",
    "plt.grid(False)\n",
    "plt.title('Inlinks grid')\n",
    "\n",
    "# Salva il grafico come file JPG con dpi=300\n",
    "plt.savefig(\"0.ingrid_visualization9gauges.jpg\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outgrid\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data_filtered['DATETIME'] = pd.to_datetime(merged_data_filtered['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 683, 695, 718,756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data = merged_data_filtered[merged_data_filtered['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.0001 else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Initialize variables to find global maxima\n",
    "global_max_links = 0\n",
    "global_max_f_statistic = 0\n",
    "station_year_data = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data['DATETIME'].dt.year.min(), merged_data['DATETIME'].dt.year.max() + 1):\n",
    "    # Filter data for months 6, 7, and 8\n",
    "    #data_year = merged_data[(merged_data['DATETIME'].dt.year == year) & \n",
    "     #                       (merged_data['DATETIME'].dt.month.isin([6, 7, 8]))]\n",
    "    \n",
    "    data_year = merged_data[(merged_data['DATETIME'].dt.year == year)]\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "\n",
    "    temp_out_links_count = {station: 0 for station in pivot_data.columns}  # Count of outlinks\n",
    "    temp_f_statistics_sum = {station: 0 for station in pivot_data.columns}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic > 0:\n",
    "                    temp_out_links_count[station1] += 1  # Counting outlinks from station1\n",
    "                    temp_f_statistics_sum[station1] += f_statistic\n",
    "                    station_year_data[(station1, year)] = (temp_f_statistics_sum[station1], temp_out_links_count[station1])  # Storing outlink data\n",
    "\n",
    "    # Update global maxima\n",
    "    year_max_links = max(temp_out_links_count.values())  # Max outlinks\n",
    "    year_max_f_statistic = max(temp_f_statistics_sum.values())\n",
    "    global_max_links = max(global_max_links, year_max_links)\n",
    "    global_max_f_statistic = max(global_max_f_statistic, year_max_f_statistic)\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {\n",
    "    779: \"TP\",\n",
    "    750: \"PA\",\n",
    "    683: \"AG\",\n",
    "    695: \"CL\",\n",
    "    718: \"EN\",\n",
    "    756: \"RG\",\n",
    "    706: \"CT\",\n",
    "    764: \"SR\",\n",
    "    729: \"ME\"\n",
    "}\n",
    "# Create the visualization of the 2D grid\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.set_facecolor('white')  # Set background to white\n",
    "\n",
    "# Define scales for size and color\n",
    "max_node_size = .2  # Maximum node size\n",
    "color_norm = plt.Normalize(0, global_max_links)  # Normalize link count\n",
    "color_map = plt.cm.coolwarm  # Color map\n",
    "\n",
    "# Draw each station-year as a circle on the grid\n",
    "for (station, year), (f_stat, links) in station_year_data.items():\n",
    "    x = year\n",
    "    y = list(station_order.keys()).index(station)  # Get station position based on order\n",
    "    size = (f_stat / global_max_f_statistic) * max_node_size  # Scale size based on f_stat\n",
    "    color = color_map(color_norm(links))  # Get color based on number of links\n",
    "\n",
    "    # Create a circle and add it to the plot\n",
    "    circle = Circle((x, y), np.sqrt(size), color=color, alpha=0.6)  # Use square root of size for radius\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "# Set axis labels, ticks, and limits\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Station ID')\n",
    "ax.set_xticks(np.arange(merged_data['DATETIME'].dt.year.min(), merged_data['DATETIME'].dt.year.max() + 1))\n",
    "ax.set_yticks(np.arange(len(station_order)))\n",
    "ax.set_yticklabels(station_order.values())\n",
    "ax.set_xlim(merged_data['DATETIME'].dt.year.min() - 1, merged_data['DATETIME'].dt.year.max() + 1)\n",
    "ax.set_ylim(-1, len(station_order))\n",
    "\n",
    "plt.grid(False)\n",
    "plt.title('Outlinks grid')\n",
    "\n",
    "# Save the plot as a JPG file with dpi=300\n",
    "plt.savefig(\"0.outgrid_visualization9gauges.jpg\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINK 2D GRID seasonal\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Haversine formula to calculate the distance between two points on the Earth\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# Function to perform Granger causality test and return F-statistic\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.0001 else 0\n",
    "    except:\n",
    "        return 0  # Return 0 in case of an error\n",
    "\n",
    "# Function to determine the color of a cell based on the link direction and distance\n",
    "def link_color(lon1, lat1, lon2, lat2, max_distance):\n",
    "    distance = haversine(lon1, lat1, lon2, lat2)\n",
    "    normalized_distance = distance / max_distance  # Normalize distance to [0, 1]\n",
    "    \n",
    "    if lon2 > lon1:\n",
    "        # Eastward link, use red colormap\n",
    "        return mcolors.to_hex(plt.cm.Reds(normalized_distance))\n",
    "    else:\n",
    "        # Westward link, use blue colormap\n",
    "        return mcolors.to_hex(plt.cm.Blues(normalized_distance))\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {\n",
    "    779: \"TP\",\n",
    "    775: \"ERICE\",\n",
    "    780: \"TRAPANI FULGATORE\",\n",
    "    778: \"SALEMI\",\n",
    "    774: \"CASTELVETRANO\",\n",
    "    773: \"CASTELLAMMARE DEL GOLFO\",\n",
    "    744: \"Contessa Entellina\",\n",
    "    751: \"PARTINICO\",\n",
    "    742: \"CAMPOREALE\",\n",
    "    749: \"MONREALE VIGNA API\",\n",
    "    745: \"CORLEONE\",\n",
    "    693: \"RIBERA\",\n",
    "    750: \"PA\",\n",
    "    686: \"BIVONA\",\n",
    "    748: \"MISILMERI\",\n",
    "    747: \"MEZZOJUSO\",\n",
    "    683: \"AG\",\n",
    "    755: \"TERMINI IMERESE\",\n",
    "    685: \"ARAGONA\",\n",
    "    684: \"AGRIGENTO MANDRASCAVA\",\n",
    "    687: \"CAMMARATA\",\n",
    "    740: \"ALIA\",\n",
    "    689: \"CANICATTì\",\n",
    "    700: \"MUSSOMELI\",\n",
    "    703: \"SCLAFANI BAGNI\",\n",
    "    690: \"LICATA\",\n",
    "    746: \"LASCARI\",\n",
    "    696: \"DELIA\",\n",
    "    754: \"POLIZZI GENEROSA\",\n",
    "    753: \"PETRALIA SOTTANA\",\n",
    "    695: \"CL\",\n",
    "    701: \"RIESI\",\n",
    "    743: \"CASTELBUONO\",\n",
    "    718: \"EN\",\n",
    "    752: \"GANGI\",\n",
    "    699: \"MAZZARINO\",\n",
    "    736: \"PETTINEO\",\n",
    "    731: \"MISTRETTA\",\n",
    "    722: \"PIAZZA ARMERINA\",\n",
    "    762: \"ACATE\",\n",
    "    721: \"NICOSIA\",\n",
    "    717: \"AIDONE\",\n",
    "    723: \"CARONIA BUZZA\",\n",
    "    760: \"SANTA CROCE CAMERINA\",\n",
    "    710: \"MAZZARRONE\",\n",
    "    716: \"CALTAGIRONE\",\n",
    "    757: \"COMISO\",\n",
    "    737: \"SAN FRATELLO\",\n",
    "    730: \"MILITELLO ROSMARINO\",\n",
    "    761: \"SCICLI\",\n",
    "    756: \"RG\",\n",
    "    725: \"CESARò VIGNAZZA\",\n",
    "    711: \"MINEO\",\n",
    "    733: \"NASO\",\n",
    "    705: \"BRONTE\",\n",
    "    712: \"PATERNò\",\n",
    "    770: \"PALAZZOLO ACREIDE\",\n",
    "    709: \"MALETTO\",\n",
    "    765: \"FRANCOFONTE\",\n",
    "    766: \"LENTINI\",\n",
    "    715: \"RANDAZZO\",\n",
    "    758: \"ISPICA\",\n",
    "    735: \"PATTI\",\n",
    "    713: \"PEDARA\",\n",
    "    767: \"NOTO\",\n",
    "    706: \"CT\",\n",
    "    769: \"PACHINO\",\n",
    "    708: \"LINGUAGLOSSA\",\n",
    "    734: \"NOVARA DI SICILIA\",\n",
    "    764: \"SR\",\n",
    "    707: \"RIPOSTO\",\n",
    "    739: \"TORREGROTTA\",\n",
    "    738: \"SAN PIER NICETO\",\n",
    "    727: \"FIUMEDINISI\",\n",
    "    729: \"ME\"\n",
    "}\n",
    "\n",
    "\n",
    "# Create a list of station IDs in order\n",
    "station_list = list(station_order.keys())\n",
    "\n",
    "# Process data for each year and calculate all F-statistics\n",
    "all_f_statistics = []\n",
    "for year in range(merged_data['DATETIME'].dt.year.min(), merged_data['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data[(merged_data['DATETIME'].dt.year == year) & (merged_data['DATETIME'].dt.month.isin([9, 10, 11]))]\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                all_f_statistics.append(f_statistic)\n",
    "\n",
    "# Calculate the 99th percentile of F-statistics\n",
    "f_statistic_99th_percentile = np.percentile(all_f_statistics, 99)\n",
    "\n",
    "# Initialize a matrix to store the link information for each year\n",
    "years = range(merged_data['DATETIME'].dt.year.min(), merged_data['DATETIME'].dt.year.max() + 1)\n",
    "num_stations = len(station_list)\n",
    "link_matrix = {year: np.zeros((num_stations, num_stations)) for year in years}\n",
    "\n",
    "# Populate the link matrix\n",
    "for year in years:\n",
    "    data_year = merged_data[(merged_data['DATETIME'].dt.year == year) & (merged_data['DATETIME'].dt.month.isin([9, 10,11]))]\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "\n",
    "    for i, station1 in enumerate(station_list):\n",
    "        for j, station2 in enumerate(station_list):\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic > f_statistic_99th_percentile:\n",
    "                    link_matrix[year][i, j] = f_statistic\n",
    "\n",
    "# Create a dictionary mapping station IDs to their coordinates\n",
    "station_coordinates = coordinates_data.set_index('STATION')[['EST', 'NORD']].to_dict('index')\n",
    "\n",
    "# Get maximum distance\n",
    "max_distance = 0\n",
    "for i in range(len(station_list)):\n",
    "    for j in range(i+1, len(station_list)):\n",
    "        lon1, lat1 = station_coordinates[station_list[i]]['EST'], station_coordinates[station_list[i]]['NORD']\n",
    "        lon2, lat2 = station_coordinates[station_list[j]]['EST'], station_coordinates[station_list[j]]['NORD']\n",
    "        dist = haversine(lon1, lat1, lon2, lat2)\n",
    "        if dist > max_distance:\n",
    "            max_distance = dist\n",
    "\n",
    "# Create and display the grids\n",
    "for year in years:\n",
    "    grid = np.ones((num_stations, num_stations, 3))  # Initialize grid with white background (no links)\n",
    "\n",
    "    for i, station1 in enumerate(station_list):\n",
    "        lon1, lat1 = station_coordinates[station1]['EST'], station_coordinates[station1]['NORD']\n",
    "        for j, station2 in enumerate(station_list):\n",
    "            if link_matrix[year][i, j] > 0:  # Check if there's a significant link\n",
    "                lon2, lat2 = station_coordinates[station2]['EST'], station_coordinates[station2]['NORD']\n",
    "                color = link_color(lon1, lat1, lon2, lat2, max_distance)\n",
    "                grid[i, j] = mcolors.to_rgb(color)\n",
    "\n",
    "    # Create and display the plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grid, interpolation='nearest')\n",
    "    plt.title(f'Autumn Adjacency Matrix for {year}')\n",
    "    plt.xticks(range(num_stations), [station_order[s] for s in station_list], rotation=90)\n",
    "    plt.yticks(range(num_stations), [station_order[s] for s in station_list])\n",
    "    plt.grid(False)\n",
    "    plt.savefig(f\"4.Grid_{year}autumn.jpg\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "    #GR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd029c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edges spatial lenght and direction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Haversine formula to calculate distance between two coordinates\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# Function to perform Granger causality test and return F-statistic\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]\n",
    "        return f_statistic if p_value < 0.0001 else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Load and prepare data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "stations_of_interest = [779, 750, 683, 695, 718, 756, 706, 764, 729]\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Define time intervals\n",
    "intervals = {\n",
    "    '2002-2012': range(2002, 2013),\n",
    "    '2013-2023': range(2013, 2024)\n",
    "}\n",
    "\n",
    "# Initialize link_matrix\n",
    "link_matrix = {}\n",
    "for interval, years in intervals.items():\n",
    "    link_matrix[interval] = np.zeros((len(stations_of_interest), len(stations_of_interest)))\n",
    "\n",
    "# Calculate F-statistics and populate link_matrix\n",
    "all_f_statistics = []\n",
    "for interval, years in intervals.items():\n",
    "    for year in years:\n",
    "        #data_year = merged_data_filtered[merged_data_filtered['DATETIME'].dt.year == year]\n",
    "        data_year = merged_data[(merged_data['DATETIME'].dt.year == year) & \n",
    "                            (merged_data['DATETIME'].dt.month.isin([6, 7, 8]))]\n",
    "    \n",
    "        pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "        \n",
    "        for i, station1 in enumerate(stations_of_interest):\n",
    "            for j, station2 in enumerate(stations_of_interest):\n",
    "                if station1 != station2:\n",
    "                    f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                    all_f_statistics.append(f_statistic)\n",
    "                    if f_statistic > np.percentile(all_f_statistics, 75):\n",
    "                        link_matrix[interval][i, j] = f_statistic\n",
    "\n",
    "# Function to count links by distance and direction\n",
    "def count_distance_direction_links(link_matrix, intervals, stations_of_interest, coordinates_data, threshold):\n",
    "    results = {}\n",
    "    station_coordinates = coordinates_data.set_index('STATION')[['EST', 'NORD']].to_dict('index')\n",
    "\n",
    "    for interval, matrix in link_matrix.items():\n",
    "        results[interval] = {'<50km': {'eastward': 0, 'westward': 0},\n",
    "                             '50-150km': {'eastward': 0, 'westward': 0},\n",
    "                             '150-250km': {'eastward': 0, 'westward': 0},\n",
    "                             '>250km': {'eastward': 0, 'westward': 0}}\n",
    "        \n",
    "        for i, station1 in enumerate(stations_of_interest):\n",
    "            for j, station2 in enumerate(stations_of_interest):\n",
    "                if i != j and matrix[i, j] > threshold:\n",
    "                    lon1, lat1 = station_coordinates[station1]['EST'], station_coordinates[station1]['NORD']\n",
    "                    lon2, lat2 = station_coordinates[station2]['EST'], station_coordinates[station2]['NORD']\n",
    "                    distance = haversine(lon1, lat1, lon2, lat2)\n",
    "                    direction = 'eastward' if lon2 > lon1 else 'westward'\n",
    "\n",
    "                    if distance < 50:\n",
    "                        results[interval]['<50km'][direction] += 1\n",
    "                    elif 50 <= distance < 150:\n",
    "                        results[interval]['50-150km'][direction] += 1\n",
    "                    elif 155 <= distance < 250:\n",
    "                        results[interval]['150-250km'][direction] += 1\n",
    "                    elif distance >= 250:\n",
    "                        results[interval]['>250km'][direction] += 1\n",
    "\n",
    "    return results\n",
    "\n",
    "# Calculate and print results for each time interval\n",
    "threshold = np.percentile(all_f_statistics, 75)\n",
    "results = count_distance_direction_links(link_matrix, intervals, stations_of_interest, coordinates_data, threshold)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658464c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decadal link counts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Haversine formula to calculate distance between two coordinates\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# Function to perform Granger causality test and return F-statistic\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]\n",
    "        return f_statistic if p_value < 0.0001 else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Load and prepare data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "stations_of_interest = [779, 750, 683, 695, 718, 756, 706, 764, 729]\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Define time intervals\n",
    "intervals = {\n",
    "    '2002-2012': range(2002, 2013),\n",
    "    '2013-2023': range(2013, 2024)\n",
    "}\n",
    "\n",
    "# Initialize link_matrix\n",
    "link_matrix = {}\n",
    "for interval, years in intervals.items():\n",
    "    link_matrix[interval] = np.zeros((len(stations_of_interest), len(stations_of_interest)))\n",
    "\n",
    "# Calculate F-statistics and populate link_matrix\n",
    "all_f_statistics = []\n",
    "for interval, years in intervals.items():\n",
    "    for year in years:\n",
    "        #data_year = merged_data_filtered[merged_data_filtered['DATETIME'].dt.year == year]\n",
    "        data_year = merged_data[(merged_data['DATETIME'].dt.year == year) & \n",
    "                            (merged_data['DATETIME'].dt.month.isin([9, 10, 11]))]\n",
    "    \n",
    "        pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "        \n",
    "        for i, station1 in enumerate(stations_of_interest):\n",
    "            for j, station2 in enumerate(stations_of_interest):\n",
    "                if station1 != station2:\n",
    "                    f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                    all_f_statistics.append(f_statistic)\n",
    "                    if f_statistic > np.percentile(all_f_statistics, 75):\n",
    "                        link_matrix[interval][i, j] = f_statistic\n",
    "\n",
    "# Function to count links by distance and direction\n",
    "def count_distance_direction_links(link_matrix, intervals, stations_of_interest, coordinates_data, threshold):\n",
    "    results = {}\n",
    "    station_coordinates = coordinates_data.set_index('STATION')[['EST', 'NORD']].to_dict('index')\n",
    "\n",
    "    for interval, matrix in link_matrix.items():\n",
    "        results[interval] = {\n",
    "                             '50-155km': {'eastward': 0, 'westward': 0},\n",
    "                             '155-350km': {'eastward': 0, 'westward': 0}\n",
    "                             }\n",
    "        \n",
    "        for i, station1 in enumerate(stations_of_interest):\n",
    "            for j, station2 in enumerate(stations_of_interest):\n",
    "                if i != j and matrix[i, j] > threshold:\n",
    "                    lon1, lat1 = station_coordinates[station1]['EST'], station_coordinates[station1]['NORD']\n",
    "                    lon2, lat2 = station_coordinates[station2]['EST'], station_coordinates[station2]['NORD']\n",
    "                    distance = haversine(lon1, lat1, lon2, lat2)\n",
    "                    direction = 'eastward' if lon2 > lon1 else 'westward'\n",
    "\n",
    "                    \n",
    "                    if 50 <= distance < 155:\n",
    "                        results[interval]['50-155km'][direction] += 1\n",
    "                    elif 155 <= distance <= 350:\n",
    "                        results[interval]['155-350km'][direction] += 1\n",
    "                   \n",
    "    return results\n",
    "\n",
    "# Calculate and print results for each time interval\n",
    "threshold = np.percentile(all_f_statistics, 75)\n",
    "results = count_distance_direction_links(link_matrix, intervals, stations_of_interest, coordinates_data, threshold)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ed0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link counts plot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Haversine formula to calculate distance between two coordinates\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# Function to perform Granger causality test and return F-statistic\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]\n",
    "        return f_statistic if p_value < 0.0001 else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Load and prepare data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "stations_of_interest = [779, 750, 683, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Initialize link_matrix\n",
    "start_year, end_year = 2002, 2023\n",
    "annual_links = {year: {'50-155km': {'eastward': 0, 'westward': 0}, '155-350km': {'eastward': 0, 'westward': 0}} for year in range(start_year, end_year+1)}\n",
    "\n",
    "all_f_statistics = []\n",
    "# Analyze data year by year\n",
    "for year in range(start_year, end_year+1):\n",
    "    data_year = merged_data[(merged_data['DATETIME'].dt.year == year) & \n",
    "                            (merged_data['DATETIME'].dt.month.isin([9, 10, 11]))]\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "\n",
    "    for i, station1 in enumerate(stations_of_interest):\n",
    "        for j, station2 in enumerate(stations_of_interest):\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                all_f_statistics.append(f_statistic)\n",
    "                if f_statistic > np.percentile(all_f_statistics, 75):\n",
    "                    lon1, lat1 = coordinates_data.loc[coordinates_data['STATION'] == station1, ['EST', 'NORD']].values[0]\n",
    "                    lon2, lat2 = coordinates_data.loc[coordinates_data['STATION'] == station2, ['EST', 'NORD']].values[0]\n",
    "                    distance = haversine(lon1, lat1, lon2, lat2)\n",
    "                    direction = 'eastward' if lon2 > lon1 else 'westward'\n",
    "                    \n",
    "                    if 50 <= distance < 155:\n",
    "                        annual_links[year]['50-155km'][direction] += 1\n",
    "                    elif 155 <= distance <= 350:\n",
    "                        annual_links[year]['155-350km'][direction] += 1\n",
    "\n",
    "# Plotting results\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "years = range(start_year, end_year+1)\n",
    "for i, dist_category in enumerate(['50-155km', '155-350km']):\n",
    "    eastward_counts = [annual_links[year][dist_category]['eastward'] for year in years]\n",
    "    westward_counts = [annual_links[year][dist_category]['westward'] for year in years]\n",
    "\n",
    "    axes[i].plot(years, eastward_counts, label='Eastward', marker='o', linestyle='-', color='red')\n",
    "    axes[i].plot(years, westward_counts, label='Westward', marker='x', linestyle='--', color='blue')\n",
    "    axes[i].set_title(f'Summer Annual Links 75th percentile for Distance Category {dist_category}')\n",
    "    axes[i].set_ylabel('Count of Links')\n",
    "    axes[i].legend()\n",
    "\n",
    "axes[-1].set_xlabel('Year')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"4.links.jpg\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59748a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of strenghts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Haversine formula to calculate distance between two coordinates\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# Function to perform Granger causality test and return F-statistic\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]\n",
    "        return f_statistic if p_value < 0.0001 else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Load and prepare data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "stations_of_interest = [779, 750, 683, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Initialize link_matrix\n",
    "start_year, end_year = 2002, 2023\n",
    "annual_links = {year: {'50-155km': {'eastward': 0, 'westward': 0}, '155-350km': {'eastward': 0, 'westward': 0}} for year in range(start_year, end_year+1)}\n",
    "\n",
    "all_f_statistics = []\n",
    "# Analyze data year by year\n",
    "for year in range(start_year, end_year+1):\n",
    "    data_year = merged_data[(merged_data['DATETIME'].dt.year == year) & \n",
    "                            (merged_data['DATETIME'].dt.month.isin([9, 10, 11]))]\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "\n",
    "    for i, station1 in enumerate(stations_of_interest):\n",
    "        for j, station2 in enumerate(stations_of_interest):\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                all_f_statistics.append(f_statistic)\n",
    "                if f_statistic > np.percentile(all_f_statistics, 75):\n",
    "                    lon1, lat1 = coordinates_data.loc[coordinates_data['STATION'] == station1, ['EST', 'NORD']].values[0]\n",
    "                    lon2, lat2 = coordinates_data.loc[coordinates_data['STATION'] == station2, ['EST', 'NORD']].values[0]\n",
    "                    distance = haversine(lon1, lat1, lon2, lat2)\n",
    "                    direction = 'eastward' if lon2 > lon1 else 'westward'\n",
    "                    \n",
    "                    if 50 <= distance < 155:\n",
    "                        annual_links[year]['50-155km'][direction] += f_statistic\n",
    "                    elif 155 <= distance <= 350:\n",
    "                        annual_links[year]['155-350km'][direction] += f_statistic\n",
    "\n",
    "# Plotting results\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "years = range(start_year, end_year+1)\n",
    "for i, dist_category in enumerate(['50-155km', '155-350km']):\n",
    "    eastward_sums = [annual_links[year][dist_category]['eastward'] for year in years]\n",
    "    westward_sums = [annual_links[year][dist_category]['westward'] for year in years]\n",
    "\n",
    "    axes[i].plot(years, eastward_sums, label='Eastward', marker='o', linestyle='-', color='red')\n",
    "    axes[i].plot(years, westward_sums, label='Westward', marker='x', linestyle='--',color='blue')\n",
    "    axes[i].set_title(f'Autumn Annual Link Strengths 75th percentile for Distance Category {dist_category}')\n",
    "    axes[i].set_ylabel('Sum of F-statistics')\n",
    "    axes[i].legend()\n",
    "\n",
    "axes[-1].set_xlabel('Year')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"4.strengths.jpg\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b7047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NETWORK ANALYISIS Clustering Coefficient and Average Shortest Path Length Across Year\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import networkx as nx\n",
    "\n",
    "# Haversine formula to calculate the distance between two points on the Earth\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# Function to perform Granger causality test and return F-statistic\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.0001 else 0\n",
    "    except:\n",
    "        return 0  # Return 0 in case of an error\n",
    "\n",
    "# Function to determine the color of a cell based on the link direction and distance\n",
    "def link_color(lon1, lat1, lon2, lat2, max_distance):\n",
    "    distance = haversine(lon1, lat1, lon2, lat2)\n",
    "    normalized_distance = distance / max_distance  # Normalize distance to [0, 1]\n",
    "    \n",
    "    if lon2 > lon1:\n",
    "        # Eastward link, use red colormap\n",
    "        return mcolors.to_hex(plt.cm.Reds(normalized_distance))\n",
    "    else:\n",
    "        # Westward link, use blue colormap\n",
    "        return mcolors.to_hex(plt.cm.Blues(normalized_distance))\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 683, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {\n",
    "    779: \"TP\",\n",
    "    750: \"PA\",\n",
    "    683: \"AG\",\n",
    "    695: \"CL\",\n",
    "    718: \"EN\",\n",
    "    756: \"RG\",\n",
    "    706: \"CT\",\n",
    "    764: \"SR\",\n",
    "    729: \"ME\"\n",
    "}\n",
    "\n",
    "# Create a list of station IDs in order\n",
    "station_list = list(station_order.keys())\n",
    "\n",
    "# Process data for each year and calculate all F-statistics\n",
    "all_f_statistics = []\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year)]\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                all_f_statistics.append(f_statistic)\n",
    "\n",
    "# Calculate the 75th percentile of F-statistics\n",
    "f_statistic_75th_percentile = np.percentile(all_f_statistics, 75)\n",
    "\n",
    "# Initialize a matrix to store the link information for each year\n",
    "years = range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1)\n",
    "num_stations = len(station_list)\n",
    "link_matrix = {year: np.zeros((num_stations, num_stations)) for year in years}\n",
    "\n",
    "# Populate the link matrix\n",
    "for year in years:\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year)]\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "    for i, station1 in enumerate(station_list):\n",
    "        for j, station2 in enumerate(station_list):\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic > f_statistic_75th_percentile:\n",
    "                    link_matrix[year][i, j] = f_statistic\n",
    "\n",
    "# Create a dictionary mapping station IDs to their coordinates\n",
    "station_coordinates = coordinates_data.set_index('STATION')[['EST', 'NORD']].to_dict('index')\n",
    "\n",
    "# Get maximum distance\n",
    "max_distance = 0\n",
    "for i in range(len(station_list)):\n",
    "    for j in range(i+1, len(station_list)):\n",
    "        lon1, lat1 = station_coordinates[station_list[i]]['EST'], station_coordinates[station_list[i]]['NORD']\n",
    "        lon2, lat2 = station_coordinates[station_list[j]]['EST'], station_coordinates[station_list[j]]['NORD']\n",
    "        dist = haversine(lon1, lat1, lon2, lat2)\n",
    "        if dist > max_distance:\n",
    "            max_distance = dist\n",
    "\n",
    "# Initialize dictionaries to store clustering coefficient and average shortest path length\n",
    "clustering_coefficient = {}\n",
    "avg_shortest_path_length = {}\n",
    "\n",
    "# Calculate clustering coefficient and average shortest path length for each year\n",
    "for year in years:\n",
    "    G = nx.Graph()\n",
    "    for station in station_list:\n",
    "        lon, lat = station_coordinates[station]['EST'], station_coordinates[station]['NORD']\n",
    "        G.add_node(station, pos=(lon, lat))\n",
    "    for i, station1 in enumerate(station_list):\n",
    "        for j, station2 in enumerate(station_list):\n",
    "            if link_matrix[year][i, j] > 0:\n",
    "                G.add_edge(station1, station2)\n",
    "    \n",
    "    if nx.is_connected(G):  # Check if the graph is connected\n",
    "        clustering_coefficient[year] = nx.average_clustering(G)\n",
    "        avg_shortest_path_length[year] = nx.average_shortest_path_length(G)\n",
    "    else:\n",
    "        # Handle the case where the graph is not connected\n",
    "        clustering_coefficient[year] = 0\n",
    "        avg_shortest_path_length[year] = float('inf')  # Set to infinity or any appropriate value\n",
    "\n",
    "# Plotting code remains the same\n",
    "\n",
    "\n",
    "# Plotting 75th percentile degree across years\n",
    "percentile_degree_75 = {}\n",
    "for year, G in network_graphs.items():\n",
    "    degree_sequence = [d for n, d in G.degree()]\n",
    "    percentile_degree_75[year] = np.percentile(degree_sequence, 75)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(percentile_degree_75.keys()), list(percentile_degree_75.values()), marker='o', linestyle='-')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('75th Percentile Degree')\n",
    "plt.title('75th Percentile Degree Across Years')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting clustering coefficient and average shortest path length across years\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(clustering_coefficient.keys()), list(clustering_coefficient.values()), marker='o', linestyle='-', label='Clustering Coefficient')\n",
    "plt.plot(list(avg_shortest_path_length.keys()), list(avg_shortest_path_length.values()), marker='o', linestyle='-', label='Average Shortest Path Length')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Clustering Coefficient and Average Shortest Path Length Across Years')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10619175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting 75th percentile degree across years\n",
    "percentile_degree_75 = {}\n",
    "for year, G in network_graphs.items():\n",
    "    degree_sequence = [d for n, d in G.degree()]\n",
    "    percentile_degree_75[year] = np.percentile(degree_sequence, 75)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(percentile_degree_75.keys()), list(percentile_degree_75.values()), marker='o', linestyle='-')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('75th Percentile Degree')\n",
    "plt.title('75th Percentile Degree Across Years')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b5962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f stats PDF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Haversine formula to calculate the distance between two points on the Earth\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# Function to perform Granger causality test and return F-statistic\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.0001 else 0\n",
    "    except:\n",
    "        return 0  # Return 0 in case of an error\n",
    "\n",
    "# Function to determine the color of a cell based on the link direction and distance\n",
    "def link_color(lon1, lat1, lon2, lat2, max_distance):\n",
    "    distance = haversine(lon1, lat1, lon2, lat2)\n",
    "    normalized_distance = distance / max_distance  # Normalize distance to [0, 1]\n",
    "    \n",
    "    if lon2 > lon1:\n",
    "        # Eastward link, use red colormap\n",
    "        return mcolors.to_hex(plt.cm.Reds(normalized_distance))\n",
    "    else:\n",
    "        # Westward link, use blue colormap\n",
    "        return mcolors.to_hex(plt.cm.Blues(normalized_distance))\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 683, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {\n",
    "    779: \"TP\",\n",
    "    750: \"PA\",\n",
    "    683: \"AG\",\n",
    "    695: \"CL\",\n",
    "    718: \"EN\",\n",
    "    756: \"RG\",\n",
    "    706: \"CT\",\n",
    "    764: \"SR\",\n",
    "    729: \"ME\"\n",
    "}\n",
    "\n",
    "# Create a list of station IDs in order\n",
    "station_list = list(station_order.keys())\n",
    "\n",
    "# Process data for each year and calculate all F-statistics\n",
    "all_f_statistics = []\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([9, 10, 11]))]\n",
    "    #data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year)]\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                all_f_statistics.append({'year': year, 'f_statistic': f_statistic})\n",
    "\n",
    "# Filter F-statistics for the two time intervals\n",
    "f_statistics_2002_2012 = [f_statistic for f_statistic in all_f_statistics if 2002 <= f_statistic['year'] <= 2012]\n",
    "f_statistics_2013_2023 = [f_statistic for f_statistic in all_f_statistics if 2013 <= f_statistic['year'] <= 2023]\n",
    "\n",
    "# Extract F-statistic values\n",
    "f_values_2002_2012 = [f_statistic['f_statistic'] for f_statistic in f_statistics_2002_2012]\n",
    "f_values_2013_2023 = [f_statistic['f_statistic'] for f_statistic in f_statistics_2013_2023]\n",
    "\n",
    "# Calculate PDFs using Gaussian Kernel Density Estimation (KDE)\n",
    "kde_2002_2012 = gaussian_kde(f_values_2002_2012)\n",
    "kde_2013_2023 = gaussian_kde(f_values_2013_2023)\n",
    "\n",
    "# Generate x values for the plot\n",
    "x_values = np.linspace(0, max(max(f_values_2002_2012), max(f_values_2013_2023)), 1000)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_values, kde_2002_2012(x_values), 'o', label='2002-2012')\n",
    "plt.plot(x_values, kde_2013_2023(x_values), 'o', label='2013-2023')\n",
    "plt.xlabel('F-statistic')\n",
    "plt.ylabel('Density')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylim(0.0001,1)\n",
    "plt.title('Probability Density Function of F-statistics')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "#plt.savefig(f\"1.Fstat.jpg\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c3ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decumulative of strenght\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Haversine formula to calculate the distance between two points on the Earth\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# Function to perform Granger causality test and return F-statistic\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.0001 else 0\n",
    "    except:\n",
    "        return 0  # Return 0 in case of an error\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 683, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Process data for each year and calculate all F-statistics\n",
    "all_f_statistics = []\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([12, 1, 2]))]\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                all_f_statistics.append({'year': year, 'f_statistic': f_statistic})\n",
    "\n",
    "# Filter F-statistics for the two time intervals\n",
    "f_statistics_2002_2012 = [f_statistic for f_statistic in all_f_statistics if 2002 <= f_statistic['year'] <= 2012]\n",
    "f_statistics_2013_2023 = [f_statistic for f_statistic in all_f_statistics if 2013 <= f_statistic['year'] <= 2023]\n",
    "\n",
    "# Extract F-statistic values\n",
    "f_values_2002_2012 = [f_statistic['f_statistic'] for f_statistic in f_statistics_2002_2012]\n",
    "f_values_2013_2023 = [f_statistic['f_statistic'] for f_statistic in f_statistics_2013_2023]\n",
    "\n",
    "# Compute ECDF for F-statistic values\n",
    "def ecdf(data):\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(1, len(data) + 1) / len(data)\n",
    "    return x, y\n",
    "\n",
    "# Compute DCDF from ECDF\n",
    "def dcdf(data):\n",
    "    x, y = ecdf(data)\n",
    "    return x, 1 - y\n",
    "\n",
    "# Calculate ECDF for F-statistic values for 2002-2012 and 2013-2023 intervals\n",
    "ecdf_f_values_2002_2012 = ecdf(f_values_2002_2012)\n",
    "ecdf_f_values_2013_2023 = ecdf(f_values_2013_2023)\n",
    "\n",
    "# Calculate DCDF for F-statistic values for 2002-2012 and 2013-2023 intervals\n",
    "dcdf_2002_2012 = dcdf(f_values_2002_2012)\n",
    "dcdf_2013_2023 = dcdf(f_values_2013_2023)\n",
    "\n",
    "# Define Q exponential distribution function with three parameters\n",
    "def q_exponential(x, alpha, beta, q):\n",
    "    return (1 - (1 - q) * alpha * x)**(1 / (1 - q)) * beta\n",
    "\n",
    "# Fit Q exponential distribution to the data\n",
    "def fit_q_exponential(data, x_values):\n",
    "    # Filter out infinite and NaN values from the data and corresponding x_values\n",
    "    valid_indices = np.isfinite(data) & np.isfinite(x_values)\n",
    "    data = data[valid_indices]\n",
    "    x_values = x_values[valid_indices]\n",
    "\n",
    "    # Define initial guess for parameters\n",
    "    alpha_init_guess = 0.9\n",
    "    #beta_init_guess = np.max(data)\n",
    "    beta_init_guess = 0.9\n",
    "    q_init_guess = 1.1\n",
    "\n",
    "    # Perform curve fitting with initial guesses\n",
    "    try:\n",
    "        popt, pcov = curve_fit(q_exponential, x_values, data, p0=[alpha_init_guess, beta_init_guess, q_init_guess])\n",
    "        return popt\n",
    "    except RuntimeError:\n",
    "        # If curve fitting fails, return None\n",
    "        return None\n",
    "\n",
    "# Fit Q exponential distribution for each time interval\n",
    "params_2002_2012 = fit_q_exponential(dcdf_2002_2012[1], dcdf_2002_2012[0])\n",
    "params_2013_2023 = fit_q_exponential(dcdf_2013_2023[1], dcdf_2013_2023[0])\n",
    "\n",
    "# Plot DCDFs and fitted Q exponential distributions for each time interval\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot for 2002-2012 interval\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(dcdf_2002_2012[0], dcdf_2002_2012[1], 'o', label=f'DCDF (q={params_2002_2012[2]:.2f})')\n",
    "if params_2002_2012 is not None:\n",
    "    plt.plot(dcdf_2002_2012[0], q_exponential(dcdf_2002_2012[0], *params_2002_2012), label=f'Q Exponential Fit (q={params_2002_2012[2]:.2f})')\n",
    "plt.xlabel('F-statistic')\n",
    "plt.ylabel('Decumulative Probability')\n",
    "plt.xscale('log')\n",
    "plt.title('2002-2012 Summer')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "\n",
    "# Plot for 2013-2023 interval\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(dcdf_2013_2023[0], dcdf_2013_2023[1], 'o', label=f'DCDF (q={params_2013_2023[2]:.2f})')\n",
    "if params_2013_2023 is not None:\n",
    "    plt.plot(dcdf_2013_2023[0], q_exponential(dcdf_2013_2023[0], *params_2013_2023), label=f'Q Exponential Fit (q={params_2013_2023[2]:.2f})')\n",
    "plt.xlabel('F-statistic')\n",
    "plt.ylabel('Decumulative Probability')\n",
    "plt.xscale('log')\n",
    "plt.title('2013-2023 Summer')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"1.Fstatdec.jpg\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
