{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aafb9344",
   "metadata": {},
   "source": [
    "Seasonal Causal network for log(increments)\n",
    "\n",
    "- Augumented Dickey-Fuller (ADF) test\n",
    "- Jarque-Bera residuals test\n",
    "- GC statistics PDF and Decumulative distribution\n",
    "- Network building\n",
    "- Synoptic grids\n",
    "- Barplot nodes strenght\n",
    "- Network dregree across years\n",
    "- Paiwise analyisis\n",
    "- Correlation Heatmaps\n",
    "  Spearmann, Kendall-Tau\n",
    "- Newtork analysis: Clustering coefficient,mean distance, diameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791209f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('75gauges.csv')\n",
    "data['DATETIME'] = pd.to_datetime(data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include\n",
    "stations_of_interest = [729, 750, 779, 718, 706, 695, 683, 764, 756]\n",
    "\n",
    "# Filter data for the stations of interest and for winter months of 2012\n",
    "filtered_data = data[(data['STATION'].isin(stations_of_interest)) & \n",
    "                     (data['DATETIME'].dt.month.isin([6, 7, 8])) &\n",
    "                     (data['DATETIME'].dt.year == 2022)]\n",
    "\n",
    "# Replace NaN values with zeros (if any exist)\n",
    "filtered_data['VALUE'].fillna(0, inplace=True)\n",
    "\n",
    "def adf_test(series):\n",
    "    \"\"\" Helper function to perform ADF test \"\"\"\n",
    "    if len(series) < 100:\n",
    "        return 'Insufficient data'\n",
    "    else:\n",
    "        result = sm.tsa.adfuller(series, autolag='AIC')\n",
    "        return {'ADF Statistic': result[0], 'p-value': result[1], 'Used Lag': result[2], 'N observations': result[3]}\n",
    "\n",
    "def analyze_data(series):\n",
    "    results = {}\n",
    "    # Test on raw data\n",
    "    results['ADF_raw'] = adf_test(series.dropna())\n",
    "    \n",
    "    # Replace zeros with a small number to avoid log of zero\n",
    "    series_small = series.replace(0, 1e-6)\n",
    "    \n",
    "    # Test on data with small values replacing zeros\n",
    "    results['ADF_small_values'] = adf_test(series_small.dropna())\n",
    "\n",
    "    # Calculate log returns on data with small values\n",
    "    log_returns = np.log(series_small / series_small.shift(1)).dropna()\n",
    "\n",
    "    # Test on log returns\n",
    "    results['ADF_log_returns'] = adf_test(log_returns)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Pivot data to prepare for analysis\n",
    "pivot_data = filtered_data.pivot(index='DATETIME', columns='STATION', values='VALUE')\n",
    "\n",
    "# Analyze each station's data and print results\n",
    "results = {}\n",
    "for station in pivot_data.columns:\n",
    "    results[station] = analyze_data(pivot_data[station])\n",
    "    print(f\"Results for Station {station}:\")\n",
    "    for key, value in results[station].items():\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239eb9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jarque-Bera residual test\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant\n",
    "from scipy.stats import jarque_bera, norm\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges24H.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the analysis\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Function to calculate log returns\n",
    "def calculate_log_returns(group):\n",
    "    log_returns = np.log(group) - np.log(group.shift(1))\n",
    "    return log_returns\n",
    "\n",
    "# Granger causality test with manual residual calculation and plot\n",
    "def granger_test_with_manual_residuals(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        # Perform Granger causality test to get lag length\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        \n",
    "        # Use lagged values for regression\n",
    "        lags = max_lag\n",
    "        X = dataframe[[column1]].shift(lags).dropna()\n",
    "        y = dataframe[column2].iloc[lags:]\n",
    "        X = X[:len(y)]\n",
    "\n",
    "        # Add constant to the model\n",
    "        X = add_constant(X)\n",
    "        \n",
    "        # Fit OLS regression model\n",
    "        model = OLS(y, X).fit()\n",
    "\n",
    "        # Calculate residuals\n",
    "        residuals = model.resid\n",
    "        \n",
    "        # Perform Jarque-Bera test for normality on residuals\n",
    "        jb_stat, jb_p_value = jarque_bera(residuals)\n",
    "        normality = \"Normal\" if jb_p_value > 0.05 else \"Not normal\"\n",
    "        \n",
    "        # Plot residuals\n",
    "        plot_residuals(residuals, normality, column1, column2)\n",
    "        \n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        \n",
    "        return f_statistic, p_value, normality\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test_with_manual_residuals: {e}\")\n",
    "        return 0, 0, \"Error\"\n",
    "\n",
    "# Function to plot residuals\n",
    "def plot_residuals(residuals, normality, column1, column2):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot histogram of residuals\n",
    "    plt.hist(residuals, bins=20, density=True, alpha=0.6, color='g', label=\"Residuals\")\n",
    "    \n",
    "    # Fit a normal distribution to the residuals\n",
    "    mu, std = norm.fit(residuals)\n",
    "    \n",
    "    # Plot the normal distribution\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    plt.plot(x, p, 'k', linewidth=2, label=\"Normal fit\")\n",
    "    \n",
    "    # Title and labels\n",
    "    plt.title(f'Residuals Distribution: {column1} -> {column2} ({normality})')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([12, 1, 2]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year['VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic, p_value, normality = granger_test_with_manual_residuals(pivot_data, station1, station2)\n",
    "                print(f\"Year: {year}, {station1} -> {station2}, F-statistic: {f_statistic}, p-value: {p_value}, Residuals: {normality}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18725c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GC-statistics plot \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a list to collect all F-statistics\n",
    "f_statistics_list = []\n",
    "\n",
    "# Loop over each year and perform the analysis again, storing F-statistics\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([12, 1, 2]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year['VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic, p_value, normality = granger_test_with_manual_residuals(pivot_data, station1, station2)\n",
    "                # Store the F-statistic in the list if it's greater than 0\n",
    "                if f_statistic > 0:\n",
    "                    f_statistics_list.append(f_statistic)\n",
    "\n",
    "# Plot the distribution of F-statistics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(f_statistics_list, bins=20, color='blue', alpha=0.7)\n",
    "plt.title('Distribution of F-statistics from Granger Causality Tests')\n",
    "plt.xlabel('F-statistic')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecfe5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decumulative\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort the F-statistics list\n",
    "f_statistics_sorted = np.sort(f_statistics_list)\n",
    "\n",
    "# Compute the decumulative probability\n",
    "decumulative_prob = 1.0 - np.arange(1, len(f_statistics_sorted) + 1) / len(f_statistics_sorted)\n",
    "\n",
    "# Plot the decumulative probability\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(f_statistics_sorted, decumulative_prob, marker='o', linestyle='-', color='blue', alpha=0.7)\n",
    "plt.title('Decumulative Probability of F-statistics from Granger Causality Tests')\n",
    "plt.xlabel('F-statistic')\n",
    "plt.ylabel('Decumulative Probability')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDF decades\n",
    "#FSTATISTICS\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Function to perform Granger causality test and return F-statistic\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return (f_statistic, p_value) if p_value < 0.0001 else (0, 1)\n",
    "    except:\n",
    "        return 0, 1  # Return 0 in case of an error\n",
    "\n",
    "# Splitting the data into two periods\n",
    "#period1_data = merged_data[(merged_data['DATETIME'].dt.year == 2003)]\n",
    "#period2_data = merged_data[(merged_data['DATETIME'].dt.year == 2013)]\n",
    "\n",
    "# Filtering the data for the specified periods\n",
    "period1_data = merged_data[((merged_data['DATETIME'].dt.year >= 2002) & (merged_data['DATETIME'].dt.year <= 2012) & \n",
    "                            (merged_data['DATETIME'].dt.month.isin([12, 1, 2])))]\n",
    "\n",
    "period2_data = merged_data[((merged_data['DATETIME'].dt.year >= 2013) & (merged_data['DATETIME'].dt.year <= 2023) & \n",
    "                            (merged_data['DATETIME'].dt.month.isin([12, 1, 2])))]\n",
    "\n",
    "\n",
    "# Function to create a network and get F-statistics for eastward and westward links\n",
    "def create_network_and_get_stats(data):\n",
    "    G = nx.Graph()\n",
    "    f_stats_eastward = []\n",
    "    f_stats_westward = []\n",
    "\n",
    "    pivot_data = data.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic, p_value = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic > 0:\n",
    "                    G.add_edge(station1, station2)\n",
    "                    if coordinates_data[coordinates_data['STATION'] == station1]['EST'].values[0] < coordinates_data[coordinates_data['STATION'] == station2]['EST'].values[0]:\n",
    "                        f_stats_eastward.append(f_statistic)\n",
    "                    else:\n",
    "                        f_stats_westward.append(f_statistic)\n",
    "    \n",
    "    return f_stats_eastward, f_stats_westward\n",
    "\n",
    "# Get F-statistics for both periods\n",
    "f_stats_eastward_period1, f_stats_westward_period1 = create_network_and_get_stats(period1_data)\n",
    "f_stats_eastward_period2, f_stats_westward_period2 = create_network_and_get_stats(period2_data)\n",
    "\n",
    "\n",
    "# Plotting the PDFs\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(f_stats_eastward_period1, label='Eastward ', bw_adjust=0.1,color='red')\n",
    "sns.kdeplot(f_stats_westward_period1, label='Westward ', bw_adjust=0.1,color='blue')\n",
    "plt.title('PDF of F-statistics 2003')\n",
    "plt.xlabel('F-statistic')\n",
    "plt.ylabel('Density')\n",
    "#plt.xlim(0,2000)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(f_stats_eastward_period2, label='Eastward ', bw_adjust=0.1,color='red')\n",
    "sns.kdeplot(f_stats_westward_period2, label='Westward ', bw_adjust=0.1,color='blue')\n",
    "plt.title('PDF of F-statistics 2019)')\n",
    "plt.xlabel('F-statistic')\n",
    "plt.ylabel('Density')\n",
    "#plt.xlim(0,2000)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('1.f_statistics_plot.jpg', format='jpg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the PDFs\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(f_stats_eastward_period1, label='Eastward ', fill=True, bw_adjust=0.2,color='red')\n",
    "sns.kdeplot(f_stats_westward_period1, label='Westward ',fill=True, bw_adjust=0.2,color='blue')\n",
    "plt.title('PDF of F-statistics 2003')\n",
    "plt.xlabel('F-statistic')\n",
    "plt.ylabel('Density')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylim(0.0001,0.1)\n",
    "plt.xlim(20,1000)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(f_stats_eastward_period2, label='Eastward ',fill=True, bw_adjust=0.2,color='red')\n",
    "sns.kdeplot(f_stats_westward_period2, label='Westward ',fill=True, bw_adjust=0.2,color='blue')\n",
    "plt.title('PDF of F-statistics 2019')\n",
    "plt.xlabel('F-statistic')\n",
    "plt.ylabel('Density')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylim(0.0001,0.1)\n",
    "plt.xlim(20,1000)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('1.f_statistics_plotLOG.jpg', format='jpg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ea62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decumulative-decades\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.05 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Initialize variables to find global maxima\n",
    "global_max_f_statistic = 0\n",
    "station_year_data = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([12, 1, 2]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year['VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    def calculate_log_returns(group):\n",
    "        log_returns = np.log(group) - np.log(group.shift(1))\n",
    "        return log_returns\n",
    "\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic > 0:\n",
    "                    station_year_data[(station1, year)] = f_statistic\n",
    "\n",
    "    # Update global maxima\n",
    "    year_max_f_statistic = max(station_year_data.values(), default=0)\n",
    "    global_max_f_statistic = max(global_max_f_statistic, year_max_f_statistic)\n",
    "\n",
    "# Separate the F-statistics into two intervals\n",
    "f_statistics_2002_2012 = [f_stat for (station, year), f_stat in station_year_data.items() if 2002 <= year <= 2012]\n",
    "f_statistics_2013_2023 = [f_stat for (station, year), f_stat in station_year_data.items() if 2013 <= year <= 2023]\n",
    "\n",
    "# Plot the PDFs and decumulative probabilities\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Define a function to plot PDF and decumulative probability\n",
    "def plot_pdf_and_deprob(ax_pdf, ax_deprob, data, label, color):\n",
    "    kde = gaussian_kde(data)\n",
    "    x = np.linspace(0, max(data), 1000)\n",
    "    pdf = kde(x)\n",
    "    deprob = 1 - np.cumsum(pdf) * (x[1] - x[0])\n",
    "    \n",
    "    ax_pdf.plot(x, pdf, label=label, color=color)\n",
    "    ax_deprob.plot(x, deprob, label=label, color=color)\n",
    "\n",
    "# Plot for the intervals\n",
    "plot_pdf_and_deprob(axes[0], axes[1], f_statistics_2002_2012, '2002-2012', 'blue')\n",
    "plot_pdf_and_deprob(axes[0], axes[1], f_statistics_2013_2023, '2013-2023', 'red')\n",
    "\n",
    "axes[0].set_title('PDF of F-statistics')\n",
    "axes[1].set_title('Decumulative Probability of F-statistics')\n",
    "axes[0].set_xlabel('F-statistic')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[1].set_xlabel('F-statistic')\n",
    "axes[1].set_ylabel('Decumulative Probability')\n",
    "axes[1].set_yscale('log')\n",
    "axes[0].set_yscale('log')\n",
    "axes[1].set_xscale('log')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('1.f_statistics_analysis.jpg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges24H.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to calculate log returns\n",
    "def calculate_log_returns(group):\n",
    "    log_returns = np.log(group) - np.log(group.shift(1))\n",
    "    return log_returns\n",
    "\n",
    "# Function to create the plot with usual arrows, color bar, and size legend\n",
    "def create_network_plot_with_usual_arrows_and_size_legend(year, links, coordinates, title, filename, outlink_strength, inlink_strength, inlink_count):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Normalize outlink and inlink strengths\n",
    "    inlink_min, inlink_max = min(inlink_count.values()), max(inlink_count.values())\n",
    "    inlink_count_normalized = {station: (value - inlink_min) / (inlink_max - inlink_min) if inlink_max != inlink_min else 0 for station, value in inlink_count.items()}\n",
    "\n",
    "    inlink_strength_min, inlink_strength_max = min(inlink_strength.values()), max(inlink_strength.values())\n",
    "\n",
    "    # Plot the nodes\n",
    "    for _, row in coordinates.iterrows():\n",
    "        if row['STATION'] in stations_of_interest:\n",
    "            station = row['STATION']\n",
    "            size = 10 + (inlink_strength[station] - inlink_strength_min) / (inlink_strength_max - inlink_strength_min) * 40  # Node size proportional to strength\n",
    "            color = plt.cm.coolwarm(inlink_count_normalized[station])  # Node color proportional to inlink count\n",
    "            ax.plot(row['EST'], row['NORD'], 'o', markersize=size, color=color)  # circle for each node\n",
    "            ax.text(row['EST'], row['NORD'], row['LOCATION'], fontsize=12, ha='right')\n",
    "\n",
    "    # Plot the directional links with usual arrows\n",
    "    for (station1, station2), f_stat in links.items():\n",
    "        coord1 = coordinates[coordinates['STATION'] == station1][['EST', 'NORD']].values[0]\n",
    "        coord2 = coordinates[coordinates['STATION'] == station2][['EST', 'NORD']].values[0]\n",
    "\n",
    "        # Draw the arrow\n",
    "        ax.annotate(\"\",\n",
    "                    xy=(coord2[0], coord2[1]), xycoords='data',\n",
    "                    xytext=(coord1[0], coord1[1]), textcoords='data',\n",
    "                    arrowprops=dict(arrowstyle=\"-|>\", color='black', lw=1))\n",
    "\n",
    "    # Add a horizontal color bar for inlink count\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.coolwarm, norm=plt.Normalize(vmin=inlink_min, vmax=inlink_max))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, orientation='horizontal', ax=ax, pad=0.02)\n",
    "    cbar.set_label('Number of links per node', fontsize=12)\n",
    "    # Set integer ticks on the color bar\n",
    "    cbar.set_ticks(range(int(inlink_min), int(inlink_max) + 1))\n",
    "    cbar.set_ticklabels(range(int(inlink_min), int(inlink_max) + 1))\n",
    "\n",
    "    # Add a size legend\n",
    "    #legend_sizes = [10, 25, 40, 55]\n",
    "    #for size in legend_sizes:\n",
    "     #   ax.plot([], [], 'o', markersize=size, color='gray', label=f'Strength: {inlink_strength_min + (size - 10) / 10 * (inlink_strength_max - inlink_strength_min):.2f}')\n",
    "    #ax.legend(loc='lower right', title='Node Size (Strength)', fontsize=10)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Filter data for the year 2002\n",
    "year = 2002\n",
    "data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                 (merged_data_filtered['DATETIME'].dt.month.isin([12, 1, 2]))]\n",
    "\n",
    "# Replace zero values with a small quantity\n",
    "small_quantity = 0.000001\n",
    "data_year['VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "# Calculate log returns\n",
    "data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "data_year.dropna(inplace=True)\n",
    "\n",
    "# Reset index to align with the original DataFrame\n",
    "data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Pivot the filtered data\n",
    "pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "links = {}\n",
    "outlink_strength = {station: 0 for station in stations_of_interest}\n",
    "inlink_strength = {station: 0 for station in stations_of_interest}\n",
    "inlink_count = {station: 0 for station in stations_of_interest}\n",
    "\n",
    "# Analysis for each station pair\n",
    "for station1 in pivot_data.columns:\n",
    "    for station2 in pivot_data.columns:\n",
    "        if station1 != station2:\n",
    "            f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "            if f_statistic_out > 0:\n",
    "                links[(station1, station2)] = f_statistic_out\n",
    "                outlink_strength[station1] += f_statistic_out\n",
    "                inlink_strength[station2] += f_statistic_out\n",
    "                inlink_count[station2] += 1\n",
    "\n",
    "# Create plot for the year 2002\n",
    "title = f'Granger Causality Network for Winter {year} - 24h'\n",
    "filename = f'gcnetwork24H_{year}.jpg'\n",
    "create_network_plot_with_usual_arrows_and_size_legend(year, links, coordinates_data, title, filename, outlink_strength, inlink_strength, inlink_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f58dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import math\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs and their order\n",
    "station_order = {  \n",
    "    756: \"RG\",\n",
    "    764: \"SR\",\n",
    "    684: \"AG\",\n",
    "    695: \"CL\",\n",
    "    718: \"EN\",\n",
    "    706: \"CT\",\n",
    "    779: \"TP\",\n",
    "    750: \"PA\",  \n",
    "    729: \"ME\"    \n",
    "} \n",
    "\n",
    "stations_of_interest = list(station_order.keys())\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.05 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to calculate log returns\n",
    "def calculate_log_returns(group):\n",
    "    log_returns = np.log(group) - np.log(group.shift(1))\n",
    "    return log_returns\n",
    "\n",
    "# Modified function to create the plot with nodes on a circle and directional arrows with colors\n",
    "def create_network_plot_circle(year, links, coordinates, station_order, title, filename):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    num_stations = len(station_order)\n",
    "    angle = 2 * math.pi / num_stations\n",
    "\n",
    "    # Calculate positions for each node on a circle\n",
    "    positions = {}\n",
    "    for i, (station, label) in enumerate(station_order.items()):\n",
    "        theta = i * angle - math.pi / 2  # Start at the top (12 o'clock)\n",
    "        x = math.cos(theta)\n",
    "        y = math.sin(theta)\n",
    "        positions[station] = (x, y)\n",
    "        ax.plot(x, y, 'bo', markersize=5)  # blue circle for each node\n",
    "        ax.text(x, y, label, fontsize=12, ha='right', va='center')\n",
    "\n",
    "    # Plot the directional links with varying thickness\n",
    "    station_keys = list(station_order.keys())\n",
    "    for (station1, station2), f_stat in links.items():\n",
    "        x1, y1 = positions[station1]\n",
    "        x2, y2 = positions[station2]\n",
    "        index1 = station_keys.index(station1)\n",
    "        index2 = station_keys.index(station2)\n",
    "        if (index2 - index1) % num_stations < num_stations // 2:\n",
    "            color = 'blue'  # Clockwise\n",
    "        else:\n",
    "            color = 'red'  # Counterclockwise\n",
    "        arrow = FancyArrowPatch((x1, y1), (x2, y2), color=color, arrowstyle='->', mutation_scale=10, lw=f_stat/10)\n",
    "        ax.add_patch(arrow)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([12, 1, 2]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year['VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    links = {}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic_out > 0:\n",
    "                    links[(station1, station2)] = f_statistic_out\n",
    "\n",
    "    # Create plot for the year\n",
    "    title = f'Granger Causality Network for Winter {year}'\n",
    "    filename = f'11.gcnetwork_{year}.jpg'\n",
    "    create_network_plot_circle(year, links, coordinates_data, station_order, title, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93f1de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMALL QUANITY SYNOPTIC GRID\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges12H.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Initialize variables to find global maxima\n",
    "global_max_out_links = 0\n",
    "global_max_in_links = 0\n",
    "global_max_f_statistic_out = 0\n",
    "global_max_f_statistic_in = 0\n",
    "station_year_data_out = {}\n",
    "station_year_data_in = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([12, 1, 2]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year.loc[:, 'VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    def calculate_log_returns(group):\n",
    "        log_returns = np.log(group) - np.log(group.shift(1))\n",
    "        return log_returns\n",
    "\n",
    "    data_year.loc[:, 'LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year = data_year.dropna()\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    temp_out_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_in_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum_out = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum_in = {station: 0 for station in pivot_data.columns}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                f_statistic_in = granger_test(pivot_data, station2, station1)\n",
    "                \n",
    "                if f_statistic_out > 0:\n",
    "                    temp_out_links_count[station1] += 1\n",
    "                    temp_f_statistics_sum_out[station1] += f_statistic_out\n",
    "                    station_year_data_out[(station1, year)] = (temp_f_statistics_sum_out[station1], temp_out_links_count[station1])\n",
    "                \n",
    "                if f_statistic_in > 0:\n",
    "                    temp_in_links_count[station1] += 1\n",
    "                    temp_f_statistics_sum_in[station1] += f_statistic_in\n",
    "                    station_year_data_in[(station1, year)] = (temp_f_statistics_sum_in[station1], temp_in_links_count[station1])\n",
    "\n",
    "    # Update global maxima\n",
    "    year_max_out_links = max(temp_out_links_count.values())\n",
    "    year_max_in_links = max(temp_in_links_count.values())\n",
    "    year_max_f_statistic_out = max(temp_f_statistics_sum_out.values())\n",
    "    year_max_f_statistic_in = max(temp_f_statistics_sum_in.values())\n",
    "    \n",
    "    global_max_out_links = max(global_max_out_links, year_max_out_links)\n",
    "    global_max_in_links = max(global_max_in_links, year_max_in_links)\n",
    "    global_max_f_statistic_out = max(global_max_f_statistic_out, year_max_f_statistic_out)\n",
    "    global_max_f_statistic_in = max(global_max_f_statistic_in, year_max_f_statistic_in)\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {\n",
    "    779: \"TP\",\n",
    "    750: \"PA\",\n",
    "    684: \"AG\",\n",
    "    695: \"CL\",\n",
    "    718: \"EN\",\n",
    "    756: \"RG\",\n",
    "    706: \"CT\",\n",
    "    764: \"SR\",\n",
    "    729: \"ME\"\n",
    "}  # ESTOVEST\n",
    "\n",
    "def create_visualization(station_year_data, global_max_links, global_max_f_statistic, title, filename, fontsize=12):\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    ax.set_facecolor('white')  # Set background to white\n",
    "    \n",
    "    # Define scales for size and color\n",
    "    max_node_size = 0.2  # Maximum node size\n",
    "    color_norm = plt.Normalize(0, global_max_links)  # Normalize link count\n",
    "    color_map = plt.cm.coolwarm  # Color map\n",
    "\n",
    "    # Draw each station-year as a circle on the grid\n",
    "    for (station, year), (f_stat, links) in station_year_data.items():\n",
    "        x = year\n",
    "        y = list(station_order.keys()).index(station)  # Get station position based on order\n",
    "        size = (f_stat / global_max_f_statistic) * max_node_size  # Scale size based on f_stat\n",
    "        color = color_map(color_norm(links))  # Get color based on number of links\n",
    "\n",
    "        # Create a circle and add it to the plot\n",
    "        circle = Circle((x, y), np.sqrt(size), color=color, alpha=0.6)  # Use square root of size for radius\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "    # Add color bar\n",
    "    sm = plt.cm.ScalarMappable(cmap=color_map, norm=color_norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, orientation='vertical', pad=0.02)\n",
    "    cbar.set_label('Number of Links', fontsize=12)\n",
    "    cbar.set_ticks(range(0, global_max_links + 1))  # Set integer ticks on the color bar\n",
    "\n",
    "    # Set axis labels, ticks, and limits\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Station ID')\n",
    "    ax.set_xticks(np.arange(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1))\n",
    "    ax.set_yticks(np.arange(len(station_order)))\n",
    "    ax.set_yticklabels(station_order.values())\n",
    "    ax.set_xlim(merged_data_filtered['DATETIME'].dt.year.min() - 1, merged_data_filtered['DATETIME'].dt.year.max() + 1)\n",
    "    ax.set_ylim(-1, len(station_order))\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.title(title, fontsize=fontsize)\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations for outlinks and inlinks\n",
    "# Create visualizations for outlinks and inlinks\n",
    "create_visualization(station_year_data_out, global_max_out_links, global_max_f_statistic_out, 'Winter Outlinks - 12h',\n",
    "                     \"1.12Hgrid_visualization9gauges.jpg\", fontsize=14)\n",
    "create_visualization(station_year_data_in, global_max_in_links, global_max_f_statistic_in, 'Winter Inlinks - 12h',\n",
    "                     \"1.12Hingrid_visualization9gauges.jpg\", fontsize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967221a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synoptic grid\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.05 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Initialize variables to find global maxima\n",
    "global_max_out_links = 0\n",
    "global_max_in_links = 0\n",
    "global_max_f_statistic_out = 0\n",
    "global_max_f_statistic_in = 0\n",
    "station_year_data_out = {}\n",
    "station_year_data_in = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([12, 1, 2]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year['VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    def calculate_log_returns(group):\n",
    "        log_returns = np.log(group) - np.log(group.shift(1))\n",
    "        return log_returns\n",
    "\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    temp_out_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_in_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum_out = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum_in = {station: 0 for station in pivot_data.columns}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                f_statistic_in = granger_test(pivot_data, station2, station1)\n",
    "                \n",
    "                if f_statistic_out > 0:\n",
    "                    temp_out_links_count[station1] += 1\n",
    "                    temp_f_statistics_sum_out[station1] += f_statistic_out\n",
    "                    station_year_data_out[(station1, year)] = (temp_f_statistics_sum_out[station1], temp_out_links_count[station1])\n",
    "                \n",
    "                if f_statistic_in > 0:\n",
    "                    temp_in_links_count[station1] += 1\n",
    "                    temp_f_statistics_sum_in[station1] += f_statistic_in\n",
    "                    station_year_data_in[(station1, year)] = (temp_f_statistics_sum_in[station1], temp_in_links_count[station1])\n",
    "\n",
    "    # Update global maxima\n",
    "    year_max_out_links = max(temp_out_links_count.values())\n",
    "    year_max_in_links = max(temp_in_links_count.values())\n",
    "    year_max_f_statistic_out = max(temp_f_statistics_sum_out.values())\n",
    "    year_max_f_statistic_in = max(temp_f_statistics_sum_in.values())\n",
    "    \n",
    "    global_max_out_links = max(global_max_out_links, year_max_out_links)\n",
    "    global_max_in_links = max(global_max_in_links, year_max_in_links)\n",
    "    global_max_f_statistic_out = max(global_max_f_statistic_out, year_max_f_statistic_out)\n",
    "    global_max_f_statistic_in = max(global_max_f_statistic_in, year_max_f_statistic_in)\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {  \n",
    "    756: \"RG\",\n",
    "    764: \"SR\",\n",
    "    684: \"AG\",\n",
    "    706: \"CT\",\n",
    "    779: \"TP\",\n",
    "    750: \"PA\",  \n",
    "     729: \"ME\"    \n",
    "} #NORDSUD\n",
    "\n",
    "# Function to create visualization of the 2D grid\n",
    "def create_visualization(station_year_data, global_max_links, global_max_f_statistic, title, filename):\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    ax.set_facecolor('white')  # Set background to white\n",
    "\n",
    "    # Define scales for size and color\n",
    "    max_node_size = 0.2  # Maximum node size\n",
    "    color_norm = plt.Normalize(0, global_max_links)  # Normalize link count\n",
    "    color_map = plt.cm.coolwarm  # Color map\n",
    "\n",
    "    # Draw each station-year as a circle on the grid\n",
    "    circles = []\n",
    "    for (station, year), (f_stat, links) in station_year_data.items():\n",
    "        x = year\n",
    "        y = list(station_order.keys()).index(station)  # Get station position based on order\n",
    "        size = (f_stat / global_max_f_statistic) * max_node_size  # Scale size based on f_stat\n",
    "        color = color_map(color_norm(links))  # Get color based on number of links\n",
    "\n",
    "        # Create a circle and add it to the plot\n",
    "        circle = Circle((x, y), np.sqrt(size), color=color, alpha=0.6)  # Use square root of size for radius\n",
    "        ax.add_patch(circle)\n",
    "        circles.append(circle)\n",
    "\n",
    "    # Set axis labels, ticks, and limits\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Station ID')\n",
    "    ax.set_xticks(np.arange(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1))\n",
    "    ax.set_yticks(np.arange(len(station_order)))\n",
    "    ax.set_yticklabels(station_order.values())\n",
    "    ax.set_xlim(merged_data_filtered['DATETIME'].dt.year.min() - 1, merged_data_filtered['DATETIME'].dt.year.max() + 1)\n",
    "    ax.set_ylim(-1, len(station_order))\n",
    "\n",
    "    # Add color bar for link count\n",
    "    sm = plt.cm.ScalarMappable(cmap=color_map, norm=color_norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax)\n",
    "    cbar.set_label('Number of Links')\n",
    "\n",
    "    # Add legend for circle sizes\n",
    "    legend_sizes = [global_max_f_statistic * x / 3 for x in range(1, 4)]  # Example sizes\n",
    "    for size in legend_sizes:\n",
    "        ax.scatter([], [], c='k', alpha=0.6, s=np.sqrt((size / global_max_f_statistic) * max_node_size) * 1000,\n",
    "                   label=f'F-stat: {size:.2f}')\n",
    "    ax.legend(scatterpoints=1, frameon=False, labelspacing=1, title='Circle Sizes')\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations for outlinks and inlinks\n",
    "create_visualization(station_year_data_out, global_max_out_links, global_max_f_statistic_out, 'Winter Outlinks', \"1.9gauges.jpg\")\n",
    "create_visualization(station_year_data_in, global_max_in_links, global_max_f_statistic_in, 'Winter Inlinks', \"1.9gauges.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f777a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot node strenght\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "#gauges_data = pd.read_csv('combined_data_9gauges.csv')\n",
    "#gauges_data = pd.read_csv('75gauges.csv')\n",
    "#gauges_data = pd.read_csv('75gauges6H.csv')\n",
    "#gauges_data = pd.read_csv('75gauges12H.csv')\n",
    "gauges_data = pd.read_csv('75gauges24H.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Initialize variables to find global maxima\n",
    "global_max_out_links = 0\n",
    "global_max_in_links = 0\n",
    "global_max_f_statistic_out = 0\n",
    "global_max_f_statistic_in = 0\n",
    "station_year_data_out = {}\n",
    "station_year_data_in = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([9, 10, 11]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year['VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    def calculate_log_returns(group):\n",
    "        log_returns = np.log(group) - np.log(group.shift(1))\n",
    "        return log_returns\n",
    "\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    temp_out_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_in_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum_out = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum_in = {station: 0 for station in pivot_data.columns}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                f_statistic_in = granger_test(pivot_data, station2, station1)\n",
    "                \n",
    "                if f_statistic_out > 0:\n",
    "                    temp_out_links_count[station1] += 1\n",
    "                    temp_f_statistics_sum_out[station1] += f_statistic_out\n",
    "                    station_year_data_out[(station1, year)] = (temp_f_statistics_sum_out[station1], temp_out_links_count[station1])\n",
    "                \n",
    "                if f_statistic_in > 0:\n",
    "                    temp_in_links_count[station1] += 1\n",
    "                    temp_f_statistics_sum_in[station1] += f_statistic_in\n",
    "                    station_year_data_in[(station1, year)] = (temp_f_statistics_sum_in[station1], temp_in_links_count[station1])\n",
    "\n",
    "    # Update global maxima\n",
    "    year_max_out_links = max(temp_out_links_count.values())\n",
    "    year_max_in_links = max(temp_in_links_count.values())\n",
    "    year_max_f_statistic_out = max(temp_f_statistics_sum_out.values())\n",
    "    year_max_f_statistic_in = max(temp_f_statistics_sum_in.values())\n",
    "    \n",
    "    global_max_out_links = max(global_max_out_links, year_max_out_links)\n",
    "    global_max_in_links = max(global_max_in_links, year_max_in_links)\n",
    "    global_max_f_statistic_out = max(global_max_f_statistic_out, year_max_f_statistic_out)\n",
    "    global_max_f_statistic_in = max(global_max_f_statistic_in, year_max_f_statistic_in)\n",
    "\n",
    "station_order = {  \n",
    "    756: \"RG\",\n",
    "    764: \"SR\",\n",
    "    684: \"AG\",\n",
    "    695: \"CL\",\n",
    "    718: \"EN\",\n",
    "    706: \"CT\",\n",
    "    779: \"TP\",\n",
    "    750: \"PA\",  \n",
    "    729: \"ME\"    \n",
    "}    \n",
    "\n",
    "station_colors = {\n",
    "    \"CT\": \"orange\",\n",
    "    \"EN\": \"orange\",\n",
    "    \"CL\": \"orange\",\n",
    "    \"ME\": \"blue\",\n",
    "    \"TP\": \"blue\",\n",
    "    \"PA\": \"blue\",\n",
    "    \"AG\": \"red\",\n",
    "    \"RG\": \"red\",\n",
    "    \"SR\": \"red\"\n",
    "}\n",
    "\n",
    "# Function to create grid of bar plots\n",
    "def create_grid_bar_plot(station_year_data, title_prefix, filename):\n",
    "    years = sorted(set(year for _, year in station_year_data.keys()))\n",
    "    num_years = len(years)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=11, ncols=2, figsize=(20, 50))\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "\n",
    "    for i, year in enumerate(years):\n",
    "        # Calculate total F-statistic for each station in the given year\n",
    "        station_totals = {station: 0 for station, _ in station_year_data.keys()}\n",
    "        for (station, y), (f_stat, links) in station_year_data.items():\n",
    "            if y == year:\n",
    "                station_totals[station] += f_stat\n",
    "\n",
    "        # Sort stations by total F-statistic\n",
    "        sorted_stations = sorted(station_totals.items(), key=lambda x: -x[1])\n",
    "\n",
    "        # Extract data for plotting\n",
    "        stations = [station_order[station[0]] for station in sorted_stations]\n",
    "        total_f_stat = [station[1] for station in sorted_stations]\n",
    "        colors = [station_colors[station_order[station[0]]] for station in sorted_stations]\n",
    "\n",
    "        # Determine the correct subplot\n",
    "        row = i % 11\n",
    "        col = i // 11\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        # Create bar plot\n",
    "        ax.bar(stations, total_f_stat, color=colors, alpha=0.6)\n",
    "        ax.set_xlabel('Station')\n",
    "        ax.set_ylabel('Total F-statistic')\n",
    "        ax.set_ylim(0,280)\n",
    "        ax.set_xticklabels(stations, rotation=45, ha='right')\n",
    "\n",
    "        ax.set_title(f'{title_prefix} {year}')\n",
    "\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Create grid bar plots for outlinks and inlinks\n",
    "create_grid_bar_plot(station_year_data_out, 'Winter Outlinks Total GC-statistics - 24h', \"4.grid24H_outlinks.jpg\")\n",
    "create_grid_bar_plot(station_year_data_in, 'Winter Inlinks Total F-statistics 24h', \"4.grid24H_inlinks.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network degree across years\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "#gauges_data = pd.read_csv('combined_data_9gauges.csv')\n",
    "#gauges_data = pd.read_csv('75gauges.csv')\n",
    "#gauges_data = pd.read_csv('75gauges6H.csv')\n",
    "#gauges_data = pd.read_csv('75gauges12H.csv')\n",
    "gauges_data = pd.read_csv('75gauges24H.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to calculate log returns\n",
    "def calculate_log_returns(group):\n",
    "    log_returns = np.log(group) - np.log(group.shift(1))\n",
    "    return log_returns\n",
    "\n",
    "# Function to create the plot with nodes and directional arrows\n",
    "def create_network_plot(year, links, coordinates, title, filename):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Plot the nodes\n",
    "    for _, row in coordinates.iterrows():\n",
    "        if row['STATION'] in stations_of_interest:\n",
    "            ax.plot(row['EST'], row['NORD'], 'bo', markersize=2)  # blue circle for each node\n",
    "            ax.text(row['EST'], row['NORD'], row['LOCATION'], fontsize=12, ha='right')\n",
    "\n",
    "    # Plot the directional links\n",
    "    for (station1, station2), f_stat in links.items():\n",
    "        coord1 = coordinates[coordinates['STATION'] == station1][['EST', 'NORD']].values[0]\n",
    "        coord2 = coordinates[coordinates['STATION'] == station2][['EST', 'NORD']].values[0]\n",
    "        ax.annotate(\"\",\n",
    "                    xy=(coord2[0], coord2[1]), xycoords='data',\n",
    "                    xytext=(coord1[0], coord1[1]), textcoords='data',\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color='blue', lw=1))\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Initialize dictionaries to hold total number of links and sum of F-statistics\n",
    "total_links = {}\n",
    "sum_statistics = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([9, 10, 11]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year.loc[:, 'VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    links = {}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic_out > 0:\n",
    "                    links[(station1, station2)] = f_statistic_out\n",
    "\n",
    "    total_links[year] = len(links)\n",
    "    sum_statistics[year] = sum(links.values())\n",
    "\n",
    "    # Create plot for the year\n",
    "    title = f'Granger Causality Network for Winter {year}'\n",
    "    filename = f'0.gcnetwork_{year}.jpg'\n",
    "    create_network_plot(year, links, coordinates_data, title, filename)\n",
    "\n",
    "# Prepare data for heatmap\n",
    "years = sorted(list(total_links.keys()))\n",
    "heatmap_data = pd.DataFrame({\n",
    "    'Total Links': [total_links[year] for year in years],\n",
    "    #'Sum of F-statistics': [sum_statistics[year] for year in years]\n",
    "}, index=years)\n",
    "\n",
    "# Plot the heatmap with squared cells and explicit normalization\n",
    "plt.figure(figsize=(12, 2))  # Adjust the figure size to make it look like strips\n",
    "max_val = max(heatmap_data.values.flatten())\n",
    "sns.heatmap(heatmap_data.T, annot=False, cmap='coolwarm', cbar_kws={'label': 'Value'}, vmin=0, vmax=max_val, square=True)\n",
    "plt.title('Autumn Granger Causality Network Links - 24h')\n",
    "plt.xlabel('Year')\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=45) \n",
    "#plt.savefig('4.total24H_Links.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot netwrok degree and statistics \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Initialize variables to find global maxima\n",
    "global_max_out_links = 0\n",
    "global_max_in_links = 0\n",
    "global_max_f_statistic_out = 0\n",
    "global_max_f_statistic_in = 0\n",
    "station_year_data_out = {}\n",
    "station_year_data_in = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([9, 10, 11]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year['VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    def calculate_log_returns(group):\n",
    "        log_returns = np.log(group) - np.log(group.shift(1))\n",
    "        return log_returns\n",
    "\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    temp_out_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_in_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum_out = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum_in = {station: 0 for station in pivot_data.columns}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                f_statistic_in = granger_test(pivot_data, station2, station1)\n",
    "                \n",
    "                if f_statistic_out > 0:\n",
    "                    temp_out_links_count[station1] += 1\n",
    "                    temp_f_statistics_sum_out[station1] += f_statistic_out\n",
    "                    station_year_data_out[(station1, year)] = (temp_f_statistics_sum_out[station1], temp_out_links_count[station1])\n",
    "                \n",
    "                if f_statistic_in > 0:\n",
    "                    temp_in_links_count[station1] += 1\n",
    "                    temp_f_statistics_sum_in[station1] += f_statistic_in\n",
    "                    station_year_data_in[(station1, year)] = (temp_f_statistics_sum_in[station1], temp_in_links_count[station1])\n",
    "\n",
    "    # Update global maxima\n",
    "    year_max_out_links = max(temp_out_links_count.values())\n",
    "    year_max_in_links = max(temp_in_links_count.values())\n",
    "    year_max_f_statistic_out = max(temp_f_statistics_sum_out.values())\n",
    "    year_max_f_statistic_in = max(temp_f_statistics_sum_in.values())\n",
    "    \n",
    "    global_max_out_links = max(global_max_out_links, year_max_out_links)\n",
    "    global_max_in_links = max(global_max_in_links, year_max_in_links)\n",
    "    global_max_f_statistic_out = max(global_max_f_statistic_out, year_max_f_statistic_out)\n",
    "    global_max_f_statistic_in = max(global_max_f_statistic_in, year_max_f_statistic_in)\n",
    "\n",
    "station_order = {  \n",
    "    756: \"RG\",\n",
    "    764: \"SR\",\n",
    "    684: \"AG\",\n",
    "    695: \"CL\",\n",
    "    718: \"EN\",\n",
    "    706: \"CT\",\n",
    "    779: \"TP\",\n",
    "    750: \"PA\",  \n",
    "    729: \"ME\"    \n",
    "}    \n",
    "\n",
    "# Function to create bar plot visualization\n",
    "def create_bar_plot(station_year_data, title, filename):\n",
    "    # Calculate total links and total F-statistic for each station\n",
    "    station_totals = {}\n",
    "    for (station, year), (f_stat, links) in station_year_data.items():\n",
    "        if station not in station_totals:\n",
    "            station_totals[station] = {'total_f_stat': 0, 'total_links': 0}\n",
    "        station_totals[station]['total_f_stat'] += f_stat\n",
    "        station_totals[station]['total_links'] += links\n",
    "\n",
    "    # Sort stations by total links and then by total F-statistic\n",
    "    sorted_stations = sorted(station_totals.items(), key=lambda x: (-x[1]['total_links'], -x[1]['total_f_stat']))\n",
    "\n",
    "    # Extract data for plotting\n",
    "    stations = [station_order[station[0]] for station in sorted_stations]\n",
    "    total_links = [station[1]['total_links'] for station in sorted_stations]\n",
    "    total_f_stat = [station[1]['total_f_stat'] for station in sorted_stations]\n",
    "\n",
    "    # Create bar plot\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 10))\n",
    "    \n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Station')\n",
    "    ax1.set_ylabel('Total Links', color=color)\n",
    "    ax1.bar(stations, total_links, color=color, alpha=0.6)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.set_xticklabels(stations, rotation=45, ha='right')\n",
    "\n",
    "    ax2 = ax1.twinx()  # Instantiate a second y-axis that shares the same x-axis\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Total F-statistic', color=color)\n",
    "    ax2.plot(stations, total_f_stat, color=color, marker='o')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Create bar plots for outlinks and inlinks\n",
    "create_bar_plot(station_year_data_out, 'Autumn Outlinks Strength', \"0.bar_plot_outlinks.jpg\")\n",
    "create_bar_plot(station_year_data_in, 'Autumn Inlinks Strength', \"0.bar_plot_inlinks.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c9cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edges direction west-east\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "#gauges_data = pd.read_csv('combined_data_9gauges.csv')\n",
    "#gauges_data = pd.read_csv('75gauges.csv')\n",
    "#gauges_data = pd.read_csv('75gauges6H.csv')\n",
    "#gauges_data = pd.read_csv('75gauges12H.csv')\n",
    "gauges_data = pd.read_csv('75gauges24H.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to calculate log returns\n",
    "def calculate_log_returns(group):\n",
    "    log_returns = np.log(group) - np.log(group.shift(1))\n",
    "    return log_returns\n",
    "\n",
    "# Initialize dictionaries to hold total number of links and sum of F-statistics\n",
    "eastward_links = {}\n",
    "westward_links = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([6, 7, 8]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year.loc[:, 'VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    links = {}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic_out > 0:\n",
    "                    links[(station1, station2)] = f_statistic_out\n",
    "\n",
    "                    # Determine if the link is eastward or westward\n",
    "                    coord1 = coordinates_data[coordinates_data['STATION'] == station1][['EST', 'NORD']].values[0]\n",
    "                    coord2 = coordinates_data[coordinates_data['STATION'] == station2][['EST', 'NORD']].values[0]\n",
    "                    if coord1[0] < coord2[0]:\n",
    "                        if year not in eastward_links:\n",
    "                            eastward_links[year] = 0\n",
    "                        eastward_links[year] += 1\n",
    "                    else:\n",
    "                        if year not in westward_links:\n",
    "                            westward_links[year] = 0\n",
    "                        westward_links[year] += 1\n",
    "\n",
    "# Prepare data for plotting\n",
    "years = sorted(list(set(westward_links.keys()).union(set(eastward_links.keys()))))\n",
    "number_of_westward_links = [westward_links.get(year, 0) for year in years]\n",
    "number_of_eastward_links = [eastward_links.get(year, 0) for year in years]\n",
    "\n",
    "# Plot the number of westward and eastward links over the years\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot westward links with a solid line\n",
    "plt.plot(years, number_of_westward_links, marker='o', linestyle='-', color='r', label='Westward Links')\n",
    "\n",
    "# Plot eastward links with a dashed line\n",
    "plt.plot(years, number_of_eastward_links, marker='o', linestyle='--', color='b', label='Eastward Links')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Autumn no of Granger Causality Links over Years (East vs West) - 24h', fontsize=16)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Number of Links', fontsize=14)\n",
    "\n",
    "# Add grid and legend\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('4.24Heast_west_links_over_years.jpg', dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d9929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonal network degree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for plotting\n",
    "years = sorted(list(set(westward_links.keys()).union(set(eastward_links.keys()))))\n",
    "total_links = [eastward_links.get(year, 0) + westward_links.get(year, 0) for year in years]\n",
    "\n",
    "# Plot the total number of links over the years\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(years, total_links, marker='o', linestyle='-', color='#FFA500')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Summer no of Granger Causality Links over Years - 24h', fontsize=16)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Total Number of Links', fontsize=14)\n",
    "\n",
    "# Add grid and legend\n",
    "plt.grid(True)\n",
    "#plt.legend(loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('3.24Htotal_links_over_years.jpg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tabula\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for DataFrame\n",
    "data = {\n",
    "    'Autumn - 24h': years,\n",
    "    'Eastward Links': [eastward_links.get(year, 0) for year in years],\n",
    "    'Westward Links': [westward_links.get(year, 0) for year in years],\n",
    "    'Total Links': [eastward_links.get(year, 0) + westward_links.get(year, 0) for year in years]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot the table and save as JPG\n",
    "fig, ax = plt.subplots(figsize=(10, 8))  # Set the size of the figure\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Create table\n",
    "table = ax.table(cellText=df.values, colLabels=df.columns, cellLoc='center', loc='center')\n",
    "\n",
    "# Adjust the font size\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "\n",
    "# Save the table as an image\n",
    "plt.savefig('4.24Hlinks_table.jpg', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the plot (optional)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max statistics\n",
    "# Find the maximum F-statistic reached in eastward and westward links\n",
    "max_f_statistic_eastward = max(eastward_links.values()) if eastward_links else 0\n",
    "max_f_statistic_westward = max(westward_links.values()) if westward_links else 0\n",
    "\n",
    "# Find the overall maximum F-statistic\n",
    "max_f_statistic = max(max_f_statistic_eastward, max_f_statistic_westward)\n",
    "\n",
    "# Count the number of links for eastward and westward directions\n",
    "num_eastward_links = len(eastward_links)\n",
    "num_westward_links = len(westward_links)\n",
    "\n",
    "# Print the maximum values and number of links\n",
    "print(f\"Maximum F-statistic (Eastward): {max_f_statistic_eastward}\")\n",
    "print(f\"Maximum F-statistic (Westward): {max_f_statistic_westward}\")\n",
    "print(f\"Overall Maximum F-statistic: {max_f_statistic}\")\n",
    "print(f\"Number of Eastward links: {num_eastward_links}\")\n",
    "print(f\"Number of Westward links: {num_westward_links}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f3be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum F-statistic reached in eastward and westward links\n",
    "max_f_statistic_eastward = max(eastward_links.values()) if eastward_links else 0\n",
    "max_f_statistic_westward = max(westward_links.values()) if westward_links else 0\n",
    "\n",
    "# Find the overall maximum F-statistic\n",
    "max_f_statistic = max(max_f_statistic_eastward, max_f_statistic_westward)\n",
    "\n",
    "# Print the maximum values\n",
    "print(f\"Maximum F-statistic (Eastward): {max_f_statistic_eastward}\")\n",
    "print(f\"Maximum F-statistic (Westward): {max_f_statistic_westward}\")\n",
    "print(f\"Overall Maximum F-statistic: {max_f_statistic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot seasonal max strenght across years\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dati di esempio per la F-statistics (puoi modificare con i tuoi dati reali)\n",
    "dati_f_statistics = {\n",
    "    'Winter': [312.3, 514.3, 498.2, 232.5, 62.7],\n",
    "    'Spring': [584.4, 531.2, 401.3, 328.6, 85.5],\n",
    "    'Summer': [1946.9, 1781.9, 953.5, 597.0, 922.0],\n",
    "    'Autumn': [506.1, 702.8, 439.5, 192.5, 143.7],\n",
    "}\n",
    "\n",
    "# Granularit temporali\n",
    "scale_temporali = ['10 min', '1 Hr', '6 Hr', '12 Hr', '24 Hr']\n",
    "\n",
    "# Colori per le stagioni\n",
    "colori = {\n",
    "    'Winter': 'b',\n",
    "    'Spring': 'g',\n",
    "    'Summer': '#FFA500',\n",
    "    'Autumn': '#FF0000',\n",
    "}\n",
    "\n",
    "# Marker per ciascun punto (puoi personalizzare i marker per ciascun punto)\n",
    "markers = {\n",
    "    'Winter': ['^', 'o', '^', 'o', 'o'],\n",
    "    'Spring': ['^', '^', '^', '^', 'o'],\n",
    "    'Summer': ['^', '^', 'o', 'o', 'o'],\n",
    "    'Autumn': ['o', '^', '^', 'o', 'o'],\n",
    "}\n",
    "\n",
    "# Est/Ovest: True rappresenta Est (pieno), False rappresenta Ovest (vuoto)\n",
    "east_west = {\n",
    "    'Winter': [True, False, False, False, False],\n",
    "    'Spring': [False, False, False, False, False],\n",
    "    'Summer': [False, False, False, False, False],\n",
    "    'Autumn': [False, False, False, False, False],\n",
    "}\n",
    "\n",
    "# Dimensione dei marker per ciascuna categoria\n",
    "size_o_markers = 10\n",
    "size_caret_markers = 12\n",
    "\n",
    "# Creare il grafico con dimensioni maggiori\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Iterare attraverso le stagioni e plottare i valori delle F-statistics per ciascuna scala temporale\n",
    "for stagione in dati_f_statistics:\n",
    "    y_values = dati_f_statistics[stagione]\n",
    "    x_values = scale_temporali\n",
    "    marker_values = markers[stagione]\n",
    "    east_west_values = east_west[stagione]\n",
    "    \n",
    "    # Plottare i punti e le linee collegate\n",
    "    for i in range(len(x_values)):\n",
    "        marker_size = size_o_markers if marker_values[i] == 'o' else size_caret_markers\n",
    "        marker_fill = colori[stagione] if east_west_values[i] else 'none'  # Pieno per Est, trasparente per Ovest\n",
    "        ax.plot(x_values[i], y_values[i], marker=marker_values[i], color=colori[stagione],\n",
    "                markerfacecolor=marker_fill, markersize=marker_size)\n",
    "    \n",
    "    # Collegare i punti con una linea\n",
    "    ax.plot(x_values, y_values, color=colori[stagione], linewidth=1)\n",
    "\n",
    "# Aggiungere dummy plot per le stagioni senza marker nella legenda\n",
    "ax.plot([], [], color='b', linewidth=1, label='Winter')\n",
    "ax.plot([], [], color='g', linewidth=1, label='Spring')\n",
    "ax.plot([], [], color='#FFA500', linewidth=1, label='Summer')\n",
    "ax.plot([], [], color='#FF0000', linewidth=1, label='Autumn')\n",
    "\n",
    "# Aggiungere una dummy plot per spiegare il significato dei marker (2002-2012 e 2013-2023)\n",
    "ax.plot([], [], marker='o', color='black', linestyle='None', markersize=size_o_markers, label='2002-2012')\n",
    "ax.plot([], [], marker='^', color='black', linestyle='None', markersize=size_caret_markers, label='2013-2023')\n",
    "\n",
    "# Aggiungere una dummy plot per spiegare il significato di Est e Ovest\n",
    "ax.plot([], [], marker='s', color='black', markerfacecolor='black', linestyle='None', label='East')\n",
    "ax.plot([], [], marker='s', color='black', markerfacecolor='none', linestyle='None', label='West')\n",
    "\n",
    "# Aggiungere titolo e etichette con dimensioni maggiori\n",
    "plt.title('F-statistics max - WE', fontsize=20)\n",
    "plt.xlabel('Time scales', fontsize=16)\n",
    "plt.ylabel('F-statistics', fontsize=16)\n",
    "\n",
    "# Aumentare dimensione etichette degli assi\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Aggiungere la leggenda fuori dal grafico con dimensioni maggiori\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=14)\n",
    "\n",
    "# Aggiungere griglia\n",
    "plt.grid(False)\n",
    "\n",
    "# Aggiungere layout regolato\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('F-statistics max - WE.jpg', format='jpg', dpi=300)\n",
    "\n",
    "# Mostrare il grafico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairwise analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to calculate log returns\n",
    "def calculate_log_returns(group):\n",
    "    log_returns = np.log(group) - np.log(group.shift(1))\n",
    "    return log_returns\n",
    "\n",
    "# Function to create the plot with nodes and directional arrows\n",
    "def create_network_plot(year, links, coordinates, title, filename):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Plot the nodes\n",
    "    for _, row in coordinates.iterrows():\n",
    "        if row['STATION'] in stations_of_interest:\n",
    "            ax.plot(row['EST'], row['NORD'], 'bo', markersize=2)  # blue circle for each node\n",
    "            ax.text(row['EST'], row['NORD'], row['LOCATION'], fontsize=12, ha='right')\n",
    "\n",
    "    # Plot the directional links\n",
    "    for (station1, station2), f_stat in links.items():\n",
    "        coord1 = coordinates[coordinates['STATION'] == station1][['EST', 'NORD']].values[0]\n",
    "        coord2 = coordinates[coordinates['STATION'] == station2][['EST', 'NORD']].values[0]\n",
    "        ax.annotate(\"\",\n",
    "                    xy=(coord2[0], coord2[1]), xycoords='data',\n",
    "                    xytext=(coord1[0], coord1[1]), textcoords='data',\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color='blue', lw=1))\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Initialize dictionary to hold F-statistics for the PA and CT stations\n",
    "f_stats_pa_ct = {}\n",
    "f_stats_ct_pa = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([12, 1, 2]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year['VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    links = {}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic_out > 0:\n",
    "                    links[(station1, station2)] = f_statistic_out\n",
    "\n",
    "                # Collect F-statistics for PA and CT stations\n",
    "                if station1 == 750 and station2 == 779:\n",
    "                    if year not in f_stats_pa_ct:\n",
    "                        f_stats_pa_ct[year] = f_statistic_out\n",
    "                if station1 == 779 and station2 == 750:\n",
    "                    if year not in f_stats_ct_pa:\n",
    "                        f_stats_ct_pa[year] = f_statistic_out\n",
    "\n",
    "    # Create plot for the year\n",
    "    title = f'Granger Causality Network for Summer {year}'\n",
    "    filename = f'0.gcnetwork_{year}.jpg'\n",
    "    create_network_plot(year, links, coordinates_data, title, filename)\n",
    "\n",
    "# Prepare data for heatmap\n",
    "years = sorted(list(f_stats_pa_ct.keys()))\n",
    "heatmap_data = pd.DataFrame({\n",
    "    'PA-TP': [f_stats_pa_ct[year] for year in years],\n",
    "    'TP-PA': [f_stats_ct_pa[year] for year in years]\n",
    "}, index=years)\n",
    "\n",
    "# Plot the heatmap with squared cells and explicit normalization\n",
    "plt.figure(figsize=(12, 2))  # Adjust the figure size to make it look like strips\n",
    "max_val = max(heatmap_data.values.flatten())\n",
    "sns.heatmap(heatmap_data.T, annot=False, cmap='coolwarm', cbar_kws={'label': 'F-statistic'}, vmin=0, vmax=max_val, square=True)\n",
    "plt.title('Winter Granger Causality F-statistics')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Direction')\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=45) \n",
    "plt.savefig('1.PATP.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edges direction north-south strenght\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "#gauges_data = pd.read_csv('combined_data_9gauges.csv')\n",
    "#gauges_data = pd.read_csv('75gauges.csv')\n",
    "#gauges_data = pd.read_csv('75gauges6H.csv')\n",
    "#gauges_data = pd.read_csv('75gauges12H.csv')\n",
    "gauges_data = pd.read_csv('75gauges24H.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to calculate log returns\n",
    "def calculate_log_returns(group):\n",
    "    log_returns = np.log(group) - np.log(group.shift(1))\n",
    "    return log_returns\n",
    "\n",
    "# Function to create the plot with nodes and directional arrows\n",
    "def create_network_plot(year, links, coordinates, title, filename):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Plot the nodes\n",
    "    for _, row in coordinates.iterrows():\n",
    "        if row['STATION'] in stations_of_interest:\n",
    "            ax.plot(row['EST'], row['NORD'], 'bo', markersize=2)  # blue circle for each node\n",
    "            ax.text(row['EST'], row['NORD'], row['LOCATION'], fontsize=12, ha='right')\n",
    "\n",
    "    # Plot the directional links\n",
    "    for (station1, station2), f_stat in links.items():\n",
    "        coord1 = coordinates[coordinates['STATION'] == station1][['EST', 'NORD']].values[0]\n",
    "        coord2 = coordinates[coordinates['STATION'] == station2][['EST', 'NORD']].values[0]\n",
    "        ax.annotate(\"\",\n",
    "                    xy=(coord2[0], coord2[1]), xycoords='data',\n",
    "                    xytext=(coord1[0], coord1[1]), textcoords='data',\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color='blue', lw=1))\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Initialize dictionaries to hold F-statistics for directional links\n",
    "northward_links = {}\n",
    "southward_links = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([9, 10, 11]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year['VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    links = {}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic_out > 0:\n",
    "                    links[(station1, station2)] = f_statistic_out\n",
    "\n",
    "                    # Collect F-statistics for northward and southward links\n",
    "                    coord1 = coordinates_data[coordinates_data['STATION'] == station1][['EST', 'NORD']].values[0]\n",
    "                    coord2 = coordinates_data[coordinates_data['STATION'] == station2][['EST', 'NORD']].values[0]\n",
    "                    if coord1[1] < coord2[1]:\n",
    "                        if year not in northward_links:\n",
    "                            northward_links[year] = 0\n",
    "                        northward_links[year] += f_statistic_out\n",
    "                    else:\n",
    "                        if year not in southward_links:\n",
    "                            southward_links[year] = 0\n",
    "                        southward_links[year] += f_statistic_out\n",
    "\n",
    "    # Create plot for the year\n",
    "    title = f'Granger Causality Network for Winter {year}'\n",
    "    filename = f'0.gcnetwork_{year}.jpg'\n",
    "    create_network_plot(year, links, coordinates_data, title, filename)\n",
    "\n",
    "# Prepare data for heatmap\n",
    "#years = sorted(list(northward_links.keys()))\n",
    "years = sorted(list(set(eastward_links.keys()).union(set(westward_links.keys()))))  # Include all years from both dictionaries\n",
    "\n",
    "\n",
    "\n",
    "heatmap_data = pd.DataFrame({\n",
    "    'Northward': [northward_links.get(year, 0) for year in years],\n",
    "    'Southward': [southward_links.get(year, 0) for year in years]\n",
    "}, index=years)\n",
    "\n",
    "\n",
    "\n",
    "# Create a mask for cells with zero values (these will be colored in black)\n",
    "mask = heatmap_data == 0\n",
    "\n",
    "# Create a custom colormap where zero values are black and the rest use 'coolwarm'\n",
    "cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "cmap_with_black = ListedColormap(['black'] + list(cmap(np.linspace(0, 1, 256))))\n",
    "\n",
    "# Plot the heatmap with masked zero values\n",
    "plt.figure(figsize=(12, 2))  # Adjust the figure size to make it look like strips\n",
    "max_val = max(heatmap_data.values.flatten())\n",
    "\n",
    "sns.heatmap(heatmap_data.T, mask=mask.T, annot=False, cmap=cmap_with_black, cbar_kws={'label': 'F-statistic'}, vmin=0, vmax=max_val, square=True)\n",
    "\n",
    "plt.title('Summer Granger Causality F-statistics - 24h')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Direction')\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=45) \n",
    "#plt.savefig('3.24HNS.jpg', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#heatmap_data = pd.DataFrame({\n",
    "#    'Northward': [northward_links[year] for year in years],\n",
    "#    'Southward': [southward_links[year] for year in years]\n",
    "#}, index=years)\n",
    "\n",
    "# Plot the heatmap with squared cells and explicit normalization\n",
    "#plt.figure(figsize=(12, 2))  # Adjust the figure size to make it look like strips\n",
    "#max_val = max(heatmap_data.values.flatten())\n",
    "#sns.heatmap(heatmap_data.T, annot=False, cmap='coolwarm', cbar_kws={'label': 'F-statistic'}, vmin=0, vmax=max_val, square=True)\n",
    "#plt.title('Autumn Granger Causality F-statistics - 12h')\n",
    "#plt.xlabel('Year')\n",
    "#plt.ylabel('Direction')\n",
    "#plt.yticks(rotation=0)\n",
    "#plt.xticks(rotation=45) \n",
    "#plt.savefig('4.12HNS.jpg', dpi=300)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e4e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum F-statistics and the number of links for both directions\n",
    "max_f_statistic_northward = max(northward_links.values()) if northward_links else 0\n",
    "max_f_statistic_southward = max(southward_links.values()) if southward_links else 0\n",
    "max_f_statistic = max(max_f_statistic_northward, max_f_statistic_southward)\n",
    "\n",
    "num_northward_links = len(northward_links)\n",
    "num_southward_links = len(southward_links)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Maximum F-statistic (Northward): {max_f_statistic_northward}\")\n",
    "print(f\"Maximum F-statistic (Southward): {max_f_statistic_southward}\")\n",
    "print(f\"Overall Maximum F-statistic: {max_f_statistic}\")\n",
    "print(f\"Number of Northward links: {num_northward_links}\")\n",
    "print(f\"Number of Southward links: {num_southward_links}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd004af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of edges across years\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "#gauges_data = pd.read_csv('combined_data_9gauges.csv')\n",
    "#gauges_data = pd.read_csv('75gauges.csv')\n",
    "#gauges_data = pd.read_csv('75gauges6H.csv')\n",
    "#gauges_data = pd.read_csv('75gauges12H.csv')\n",
    "gauges_data = pd.read_csv('75gauges24H.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to calculate log returns\n",
    "def calculate_log_returns(group):\n",
    "    log_returns = np.log(group) - np.log(group.shift(1))\n",
    "    return log_returns\n",
    "\n",
    "# Function to create the plot with nodes and directional arrows\n",
    "def create_network_plot(year, links, coordinates, title, filename):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Plot the nodes\n",
    "    for _, row in coordinates.iterrows():\n",
    "        if row['STATION'] in stations_of_interest:\n",
    "            ax.plot(row['EST'], row['NORD'], 'bo', markersize=2)  # blue circle for each node\n",
    "            ax.text(row['EST'], row['NORD'], row['LOCATION'], fontsize=12, ha='right')\n",
    "\n",
    "    # Plot the directional links\n",
    "    for (station1, station2), f_stat in links.items():\n",
    "        coord1 = coordinates[coordinates['STATION'] == station1][['EST', 'NORD']].values[0]\n",
    "        coord2 = coordinates[coordinates['STATION'] == station2][['EST', 'NORD']].values[0]\n",
    "        ax.annotate(\"\",\n",
    "                    xy=(coord2[0], coord2[1]), xycoords='data',\n",
    "                    xytext=(coord1[0], coord1[1]), textcoords='data',\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color='blue', lw=1))\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Initialize dictionaries to hold the number of links for directional links\n",
    "northward_links_count = {}\n",
    "southward_links_count = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([9, 10, 11]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year['VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    links = {}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic_out > 0:\n",
    "                    links[(station1, station2)] = f_statistic_out\n",
    "\n",
    "                    # Count northward and southward links\n",
    "                    coord1 = coordinates_data[coordinates_data['STATION'] == station1][['EST', 'NORD']].values[0]\n",
    "                    coord2 = coordinates_data[coordinates_data['STATION'] == station2][['EST', 'NORD']].values[0]\n",
    "                    if coord1[1] < coord2[1]:\n",
    "                        if year not in northward_links_count:\n",
    "                            northward_links_count[year] = 0\n",
    "                        northward_links_count[year] += 1\n",
    "                    else:\n",
    "                        if year not in southward_links_count:\n",
    "                            southward_links_count[year] = 0\n",
    "                        southward_links_count[year] += 1\n",
    "\n",
    "    # Create plot for the year\n",
    "    title = f'Granger Causality Network for Winter {year}'\n",
    "    filename = f'0.gcnetwork_{year}.jpg'\n",
    "    create_network_plot(year, links, coordinates_data, title, filename)\n",
    "\n",
    "# Prepare data for heatmap\n",
    "#years = sorted(list(northward_links_count.keys()))\n",
    "years = sorted(list(set(eastward_links.keys()).union(set(westward_links.keys()))))  # Include all years from both dictionaries\n",
    "\n",
    "heatmap_data = pd.DataFrame({\n",
    "    'Northward': [northward_links.get(year, 0) for year in years],\n",
    "    'Southward': [southward_links.get(year, 0) for year in years]\n",
    "}, index=years)\n",
    "\n",
    "\n",
    "\n",
    "# Create a mask for cells with zero values (these will be colored in black)\n",
    "mask = heatmap_data == 0\n",
    "\n",
    "# Create a custom colormap where zero values are black and the rest use 'coolwarm'\n",
    "cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "cmap_with_black = ListedColormap(['black'] + list(cmap(np.linspace(0, 1, 256))))\n",
    "\n",
    "\n",
    "# Plot the heatmap with squared cells and explicit normalization\n",
    "plt.figure(figsize=(12, 2))  # Adjust the figure size to make it look like strips\n",
    "max_val = max(heatmap_data.values.flatten())\n",
    "sns.heatmap(heatmap_data.T, annot=False, cmap='coolwarm', cbar_kws={'label': 'Number of Links'}, vmin=0, vmax=max_val, square=True)\n",
    "plt.title('Autumn Granger Causality Number of Links - 24h')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Direction')\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=45) \n",
    "plt.savefig('44.24HNS_Links.jpg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b55e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spearman correlation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to calculate log returns\n",
    "def calculate_log_returns(group):\n",
    "    log_returns = np.log(group) - np.log(group.shift(1))\n",
    "    return log_returns\n",
    "\n",
    "adjacency_matrices = []\n",
    "years = []\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([12, 1, 2]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year['VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    adjacency_matrix = np.zeros((len(stations_of_interest), len(stations_of_interest)))\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for i, station1 in enumerate(pivot_data.columns):\n",
    "        for j, station2 in enumerate(pivot_data.columns):\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic_out > 0:\n",
    "                    adjacency_matrix[i, j] = f_statistic_out\n",
    "\n",
    "    adjacency_matrices.append(adjacency_matrix)\n",
    "    years.append(year)\n",
    "\n",
    "# Calculate Pearson and Spearman correlations for each year's adjacency matrix\n",
    "pearson_correlations = np.zeros((len(years), len(years)))\n",
    "spearman_correlations = np.zeros((len(years), len(years)))\n",
    "\n",
    "for i in range(len(years)):\n",
    "    for j in range(len(years)):\n",
    "        pearson_correlations[i, j] = np.corrcoef(adjacency_matrices[i].flatten(), adjacency_matrices[j].flatten())[0, 1]\n",
    "        spearman_correlations[i, j] = np.corrcoef(np.argsort(np.argsort(adjacency_matrices[i].flatten())),\n",
    "                                                  np.argsort(np.argsort(adjacency_matrices[j].flatten())))[0, 1]\n",
    "\n",
    "# Plot heatmaps for Pearson and Spearman correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pearson_correlations, xticklabels=years, yticklabels=years, annot=False, cmap='coolwarm', cbar=True)\n",
    "plt.title('Winter Pearson Correlation')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Year')\n",
    "plt.savefig('1.pearson_correlation_heatmap.jpg', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(spearman_correlations, xticklabels=years, yticklabels=years, annot=False, cmap='coolwarm', cbar=True)\n",
    "plt.title('Winter Spearman Correlation')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Year')\n",
    "plt.savefig('1.spearman_correlation_heatmap.jpg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kendall-tau correlation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import seaborn as sns\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to calculate log returns\n",
    "def calculate_log_returns(group):\n",
    "    log_returns = np.log(group) - np.log(group.shift(1))\n",
    "    return log_returns\n",
    "\n",
    "adjacency_matrices = []\n",
    "years = []\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([12, 1, 2]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year['VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    adjacency_matrix = np.zeros((len(stations_of_interest), len(stations_of_interest)))\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for i, station1 in enumerate(pivot_data.columns):\n",
    "        for j, station2 in enumerate(pivot_data.columns):\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic_out > 0:\n",
    "                    adjacency_matrix[i, j] = f_statistic_out\n",
    "\n",
    "    adjacency_matrices.append(adjacency_matrix)\n",
    "    years.append(year)\n",
    "\n",
    "# Initialize matrices for correlations\n",
    "kendall_correlations = np.zeros((len(years), len(years)))\n",
    "\n",
    "# Calculate correlations for each year's adjacency matrix\n",
    "for i in range(len(years)):\n",
    "    for j in range(len(years)):\n",
    "        if i != j:\n",
    "            kendall_correlations[i, j] = kendalltau(adjacency_matrices[i].flatten(), adjacency_matrices[j].flatten())[0]\n",
    "            \n",
    "        else:\n",
    "            kendall_correlations[i, j] = 1\n",
    "           \n",
    "\n",
    " # Plot heatmaps for each correlation type\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(kendall_correlations, xticklabels=years, yticklabels=years, annot=False, cmap='coolwarm', cbar=True)\n",
    "plt.title('Winter Kendall Correlation')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Year')\n",
    "plt.savefig('1.kendall_correlation_heatmap.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b45c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace 0 with mean value\n",
    "#MEAN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.05 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Initialize variables to find global maxima\n",
    "global_max_out_links = 0\n",
    "global_max_in_links = 0\n",
    "global_max_f_statistic_out = 0\n",
    "global_max_f_statistic_in = 0\n",
    "station_year_data_out = {}\n",
    "station_year_data_in = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([3, 4, 5]))]\n",
    "\n",
    "    # Calculate mean values for each station across the specified months\n",
    "    station_means = data_year[data_year['VALUE'] != 0].groupby('STATION')['VALUE'].mean()\n",
    "\n",
    "    # Replace zero values with the station's mean for the considered months\n",
    "    data_year['VALUE'] = data_year.apply(lambda row: station_means[row['STATION']] if row['VALUE'] == 0 else row['VALUE'], axis=1)\n",
    "\n",
    "    # Calculate log returns\n",
    "    def calculate_log_returns(group):\n",
    "        log_returns = np.log(group) - np.log(group.shift(1))\n",
    "        return log_returns\n",
    "\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    temp_out_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_in_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum_out = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum_in = {station: 0 for station in pivot_data.columns}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                f_statistic_in = granger_test(pivot_data, station2, station1)\n",
    "                \n",
    "                if f_statistic_out > 0:\n",
    "                    temp_out_links_count[station1] += 1\n",
    "                    temp_f_statistics_sum_out[station1] += f_statistic_out\n",
    "                    station_year_data_out[(station1, year)] = (temp_f_statistics_sum_out[station1], temp_out_links_count[station1])\n",
    "                \n",
    "                if f_statistic_in > 0:\n",
    "                    temp_in_links_count[station1] += 1\n",
    "                    temp_f_statistics_sum_in[station1] += f_statistic_in\n",
    "                    station_year_data_in[(station1, year)] = (temp_f_statistics_sum_in[station1], temp_in_links_count[station1])\n",
    "\n",
    "    # Update global maxima\n",
    "    year_max_out_links = max(temp_out_links_count.values())\n",
    "    year_max_in_links = max(temp_in_links_count.values())\n",
    "    year_max_f_statistic_out = max(temp_f_statistics_sum_out.values())\n",
    "    year_max_f_statistic_in = max(temp_f_statistics_sum_in.values())\n",
    "    \n",
    "    global_max_out_links = max(global_max_out_links, year_max_out_links)\n",
    "    global_max_in_links = max(global_max_in_links, year_max_in_links)\n",
    "    global_max_f_statistic_out = max(global_max_f_statistic_out, year_max_f_statistic_out)\n",
    "    global_max_f_statistic_in = max(global_max_f_statistic_in, year_max_f_statistic_in)\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {  \n",
    "    756: \"RG\",\n",
    "    764: \"SR\",\n",
    "    684: \"AG\",\n",
    "    695: \"CL\",\n",
    "    718: \"EN\",\n",
    "    706: \"CT\",\n",
    "    779: \"TP\",\n",
    "    750: \"PA\",  \n",
    "    729: \"ME\"    \n",
    "} #NOEDSUD\n",
    "\n",
    "\n",
    "#station_order = {\n",
    "#    779: \"TP\",\n",
    "#    750: \"PA\",\n",
    "#    684: \"AG\",\n",
    "#    695: \"CL\",\n",
    "#    718: \"EN\",\n",
    "#    756: \"RG\",\n",
    "#    706: \"CT\",\n",
    "#    764: \"SR\",\n",
    "#    729: \"ME\"\n",
    "    \n",
    "#} #ESTOVEST\n",
    "\n",
    "# Function to create visualization of the 2D grid\n",
    "def create_visualization(station_year_data, global_max_links, global_max_f_statistic, title, filename):\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    ax.set_facecolor('white')  # Set background to white\n",
    "\n",
    "    # Define scales for size and color\n",
    "    max_node_size = 0.2  # Maximum node size\n",
    "    color_norm = plt.Normalize(0, global_max_links)  # Normalize link count\n",
    "    color_map = plt.cm.coolwarm  # Color map\n",
    "\n",
    "    # Draw each station-year as a circle on the grid\n",
    "    for (station, year), (f_stat, links) in station_year_data.items():\n",
    "        x = year\n",
    "        y = list(station_order.keys()).index(station)  # Get station position based on order\n",
    "        size = (f_stat / global_max_f_statistic) * max_node_size  # Scale size based on f_stat\n",
    "        color = color_map(color_norm(links))  # Get color based on number of links\n",
    "\n",
    "        # Create a circle and add it to the plot\n",
    "        circle = Circle((x, y), np.sqrt(size), color=color, alpha=0.6)  # Use square root of size for radius\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "    # Set axis labels, ticks, and limits\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Station ID')\n",
    "    ax.set_xticks(np.arange(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1))\n",
    "    ax.set_yticks(np.arange(len(station_order)))\n",
    "    ax.set_yticklabels(station_order.values())\n",
    "    ax.set_xlim(merged_data_filtered['DATETIME'].dt.year.min() - 1, merged_data_filtered['DATETIME'].dt.year.max() + 1)\n",
    "    ax.set_ylim(-1, len(station_order))\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations for outlinks and inlinks\n",
    "create_visualization(station_year_data_out, global_max_out_links, global_max_f_statistic_out, 'Outlinks Grid Visualization', \"3.MEANoutgrid_visualization9gauges.jpg\")\n",
    "create_visualization(station_year_data_in, global_max_in_links, global_max_f_statistic_in, 'Inlinks Grid Visualization', \"3.MEANingrid_visualization9gauges.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078794a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural time\n",
    "#NATURAL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.05 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Initialize variables to find global maxima\n",
    "global_max_out_links = 0\n",
    "global_max_in_links = 0\n",
    "global_max_f_statistic_out = 0\n",
    "global_max_f_statistic_in = 0\n",
    "station_year_data_out = {}\n",
    "station_year_data_in = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([6, 7 ,8, 9, 10, 11]))]\n",
    "\n",
    "    # Remove zero values\n",
    "    data_year = data_year[data_year['VALUE'] != 0]\n",
    "\n",
    "    # Calculate log returns\n",
    "    def calculate_log_returns(group):\n",
    "        log_returns = np.log(group) - np.log(group.shift(1))\n",
    "        return log_returns\n",
    "\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    temp_out_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_in_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum_out = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum_in = {station: 0 for station in pivot_data.columns}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                f_statistic_in = granger_test(pivot_data, station2, station1)\n",
    "                \n",
    "                if f_statistic_out > 0:\n",
    "                    temp_out_links_count[station1] += 1\n",
    "                    temp_f_statistics_sum_out[station1] += f_statistic_out\n",
    "                    station_year_data_out[(station1, year)] = (temp_f_statistics_sum_out[station1], temp_out_links_count[station1])\n",
    "                \n",
    "                if f_statistic_in > 0:\n",
    "                    temp_in_links_count[station1] += 1\n",
    "                    temp_f_statistics_sum_in[station1] += f_statistic_in\n",
    "                    station_year_data_in[(station1, year)] = (temp_f_statistics_sum_in[station1], temp_in_links_count[station1])\n",
    "\n",
    "    # Update global maxima\n",
    "    year_max_out_links = max(temp_out_links_count.values())\n",
    "    year_max_in_links = max(temp_in_links_count.values())\n",
    "    year_max_f_statistic_out = max(temp_f_statistics_sum_out.values())\n",
    "    year_max_f_statistic_in = max(temp_f_statistics_sum_in.values())\n",
    "    \n",
    "    global_max_out_links = max(global_max_out_links, year_max_out_links)\n",
    "    global_max_in_links = max(global_max_in_links, year_max_in_links)\n",
    "    global_max_f_statistic_out = max(global_max_f_statistic_out, year_max_f_statistic_out)\n",
    "    global_max_f_statistic_in = max(global_max_f_statistic_in, year_max_f_statistic_in)\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {  \n",
    "    756: \"RG\",\n",
    "    764: \"SR\",\n",
    "    684: \"AG\",\n",
    "    695: \"CL\",\n",
    "    718: \"EN\",\n",
    "    706: \"CT\",\n",
    "    779: \"TP\",\n",
    "    750: \"PA\",  \n",
    "     729: \"ME\"    \n",
    "} #NOEDSUD\n",
    "\n",
    "\n",
    "#station_order = {\n",
    "#    779: \"TP\",\n",
    "#    750: \"PA\",\n",
    "#    684: \"AG\",\n",
    "#    695: \"CL\",\n",
    "#    718: \"EN\",\n",
    "#    756: \"RG\",\n",
    "#    706: \"CT\",\n",
    "#    764: \"SR\",\n",
    "#    729: \"ME\"\n",
    "    \n",
    "#} #ESTOVEST\n",
    "\n",
    "# Function to create visualization of the 2D grid\n",
    "def create_visualization(station_year_data, global_max_links, global_max_f_statistic, title, filename):\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    ax.set_facecolor('white')  # Set background to white\n",
    "\n",
    "    # Define scales for size and color\n",
    "    max_node_size = 0.2  # Maximum node size\n",
    "    color_norm = plt.Normalize(0, global_max_links)  # Normalize link count\n",
    "    color_map = plt.cm.coolwarm  # Color map\n",
    "\n",
    "    # Draw each station-year as a circle on the grid\n",
    "    for (station, year), (f_stat, links) in station_year_data.items():\n",
    "        x = year\n",
    "        y = list(station_order.keys()).index(station)  # Get station position based on order\n",
    "        size = (f_stat / global_max_f_statistic) * max_node_size  # Scale size based on f_stat\n",
    "        color = color_map(color_norm(links))  # Get color based on number of links\n",
    "\n",
    "        # Create a circle and add it to the plot\n",
    "        circle = Circle((x, y), np.sqrt(size), color=color, alpha=0.6)  # Use square root of size for radius\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "    # Set axis labels, ticks, and limits\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Station ID')\n",
    "    ax.set_xticks(np.arange(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1))\n",
    "    ax.set_yticks(np.arange(len(station_order)))\n",
    "    ax.set_yticklabels(station_order.values())\n",
    "    ax.set_xlim(merged_data_filtered['DATETIME'].dt.year.min() - 1, merged_data_filtered['DATETIME'].dt.year.max() + 1)\n",
    "    ax.set_ylim(-1, len(station_order))\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations for outlinks and inlinks\n",
    "create_visualization(station_year_data_out, global_max_out_links, global_max_f_statistic_out, 'Outlinks Grid Visualization', \"4.1NATUoutgrid_visualization9gauges.jpg\")\n",
    "create_visualization(station_year_data_in, global_max_in_links, global_max_f_statistic_in, 'Inlinks Grid Visualization', \"4.1NATUingrid_visualization9gauges.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering coefficient,mean distance, diameter\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Function to perform Granger causality test\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.0001 else 0\n",
    "    except:\n",
    "        return 0  # Return 0 in case of an error\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Years range\n",
    "years = range(merged_data['DATETIME'].dt.year.min(), merged_data['DATETIME'].dt.year.max() + 1)\n",
    "\n",
    "# Initialize lists for global properties and dictionary for degree distributions\n",
    "global_clustering_coefficients = []\n",
    "global_mean_distances = []\n",
    "global_network_diameters = []\n",
    "degree_distributions = {}\n",
    "\n",
    "# Main analysis loop\n",
    "for year in years:\n",
    "    # Filter for the specific year and months\n",
    "    data_year = merged_data[(merged_data['DATETIME'].dt.year == year) & (merged_data['DATETIME'].dt.month.isin([9, 10, 11]))]\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "\n",
    "    # Initialize NetworkX graph for this year\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Build the network with all available stations\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic > 0:\n",
    "                    G.add_edge(station1, station2)\n",
    "\n",
    "    # Calculate clustering coefficient, mean path length, and diameter\n",
    "    clustering_coefficient = nx.average_clustering(G)\n",
    "    if nx.is_connected(G):\n",
    "        mean_distance = nx.average_shortest_path_length(G)\n",
    "        diameter = nx.diameter(G)\n",
    "    else:\n",
    "        mean_distance = diameter = float('inf')\n",
    "    \n",
    "    global_clustering_coefficients.append(clustering_coefficient)\n",
    "    global_mean_distances.append(mean_distance)\n",
    "    global_network_diameters.append(diameter)\n",
    "\n",
    "    # Compute degree distribution for this year's network\n",
    "    degrees = [degree for node, degree in G.degree()]\n",
    "    degree_distributions[year] = degrees\n",
    "\n",
    "# Plotting global network properties across years\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(years, global_clustering_coefficients, marker='o')\n",
    "plt.title('Autumn Clustering Coefficient Across Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Clustering Coefficient')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(years, global_mean_distances, marker='o')\n",
    "plt.title('Autumn Mean Distance Across Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean Distance')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(years, global_network_diameters, marker='o')\n",
    "plt.title('Autumn Network Diameter Across Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Diameter')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"4.network_analysis.jpg\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
